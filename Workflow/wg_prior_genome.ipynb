{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# TWAS multivariate susie\n",
    "\n",
    "This notebook implements a TWAS analysis workflow using multivariate susie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview (TBD)\n",
    "\n",
    "__Objective__: \n",
    "    To Compute the association between expression and SNP in prepare for TWAS analysis via genotype and multiple molecular phenotype\n",
    "\n",
    "__Background__:\n",
    "    SNP can modulate the functional phenotypes both directly and by modulating the expression levels of genes. \n",
    "Therefore, the integration of expression measurements and a larger scale GWAS summary association statistics will help identify the genes associated with the targeted complex traits. \n",
    "\n",
    "__Significance__:\n",
    "    By applying this method, new candidate genes whose expression level is significantly associated with complex traits can be used in prediction without actually going through the expensive gene expression measurement process. As a relatively small set of gene expression and genotyping, data can be used to impute the expression for a much larger set of phenotyped individuals from their SNP genotype data. \n",
    "\n",
    "__Method__:\n",
    "    The imputed expression can then be viewed as a linear model of genotypes with _weights based on the correlation between SNPs and gene expression__ in the training data while accounting for linkage disequilibrium (LD) SNPs. We then correlated the imputed gene expression to the trait to perform a transcriptome-wide association study (TWAS) and identify significant expression-trait associations. \n",
    " \n",
    "The weights are computed via multivariate susies, the accuracy of such weights are computed via by default 100 times five fold cross validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "We provide a container image `docker://gaow/twas` that contains all software needed to run the pipeline. If you would like to configure it by yourself, please make sure you install the following software before running this notebook:\n",
    "- tidyverse\n",
    "- PLINK\n",
    "- R package mashr\n",
    "- R package mmbr\n",
    "- Output from the following univatiate analysis pipeline: twas_fusion_susie.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Input and Output(TBD)\n",
    "## Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Command interface (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Working example \n",
    "A minimal working example (MWE) dataset that can be downloaded from the following link, which required a synapse account:\n",
    "https://www.synapse.org/#!Synapse:syn24179065\n",
    "\n",
    "To test the command, please download and decompress the mwe folder, copy this file in it, and run the following command within the mwe folder.\n",
    "\n",
    "Alternativly, the options below can be changed based on respective relative paths.\n",
    "\n",
    "The time it take to run this MWE shall be around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "ename": "ParsingError",
     "evalue": "File contains parsing errors: \n\t[line  2]: sos run ./mv_susie.ipynb mv_susie_cv --cv_times 2  \\\n--molecular_pheno_dir \"molecular_phenotype_list\"   \\\n--region_list region_list  \\\n--wd ./   \\\n--name_prefix \"geneTpmResidualsAgeGenderAdj_rename\" \\--container /mnt/mfs/statgen/containers/twas_latest.sif --impute TRUE &\n\nInvalid statements: SyntaxError('invalid syntax', ('<string>', 1, 5, 'sos run ./mv_susie.ipynb mv_susie_cv --cv_times 2  \\\\\\n'))",
     "execution_count": 2,
     "output_type": "error",
     "status": "error",
     "traceback": [
      "\u001b[91mFile contains parsing errors: \n\t[line  2]: sos run ./mv_susie.ipynb mv_susie_cv --cv_times 2  \\\n--molecular_pheno_dir \"molecular_phenotype_list\"   \\\n--region_list region_list  \\\n--wd ./   \\\n--name_prefix \"geneTpmResidualsAgeGenderAdj_rename\" \\--container /mnt/mfs/statgen/containers/twas_latest.sif --impute TRUE &\n\nInvalid statements: SyntaxError('invalid syntax', ('<string>', 1, 5, 'sos run ./mv_susie.ipynb mv_susie_cv --cv_times 2  \\\\\\n'))\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Test the pipeline with MWE\n",
    "\n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/wg_prior_genome.ipynb mm_prior \\\n",
    "--molecular_pheno_dir /home/hs3163/Project/Genome_prior/data/molc_dir    \\\n",
    "--rds_list /home/hs3163/Project/Genome_prior/data/rds_list  \\\n",
    "--wd   /home/hs3163/Project/Genome_prior/merge \\\n",
    "--name \"geneTpmResidualsAgeGenderAdj_rename\" \\\n",
    "--container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "-J 10 -q csg -c /mnt/mfs/statgen/pbs_template/csg.yml -s build &\n",
    "\n",
    "\n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/wg_prior_genome.ipynb mm_prior \\\n",
    "--molecular_pheno_dir /home/hs3163/Project/Genome_prior/data/molc_dir    \\\n",
    "--rds_list /home/hs3163/Project/Genome_prior/data/rds_list_test  \\\n",
    "--wd   /home/hs3163/Project/Genome_prior/test \\\n",
    "--name \"geneTpmResidualsAgeGenderAdj_rename\" \\\n",
    "--container /mnt/mfs/statgen/containers/twas_latest.sif -s build &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Global parameter settings\n",
    "The section outlined the parameters that can be set in the command interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to a list of folders in which the rds are to be analysised\n",
    "parameter: molecular_pheno_dir = path\n",
    "\n",
    "# Path to a file that lists all the rds to be combined and analysis\n",
    "parameter: rds_list = path\n",
    "# Path to the work directory of this pipeline,where the output will be stored.\n",
    "parameter: wd = path\n",
    "# Specify the number of jobs per run.\n",
    "parameter: job_size = 2\n",
    "# Container option for software to run the analysis: docker or singularity\n",
    "parameter: container = 'gaow/twas'\n",
    "\n",
    "# Input data directory \n",
    "parameter: datadir = path('{wd:a}/input')\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path('{wd:a}/output')\n",
    "# The filename prefix for output data\n",
    "parameter: name = str\n",
    "# handle N = per_chunk data-set in one job\n",
    "parameter: per_chunk = 1000\n",
    "\n",
    "\n",
    "import glob\n",
    "# Get rds of interest to focus on.\n",
    "regions = [x.strip().split() for x in open(rds_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "molecular_pheno = [x.strip().split() for x in open(molecular_pheno_dir).readlines() if x.strip() and not x.strip().startswith('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Merge of X (plink) and Y (R)\n",
    "Creat merge list, and then merged based on merged list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "kernel": "SoS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mm_prior_1,merge_1]\n",
    "input:  molecular_pheno_dir, for_each = \"regions\"\n",
    "output: f'{wd:a}/input/{_regions[0]}'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'  \n",
    "\n",
    "R: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"readr\")\n",
    "    molecular_pheno = read_delim(\"$[molecular_pheno_dir]\",delim = \"\\t\")\n",
    "    molecular_pheno = molecular_pheno%>%mutate(dir = map_chr(`#molc_pheno`,~paste(c(`.x`,\"$[_regions[0]]\"),collapse = \"\")))\n",
    "    n = nrow(molecular_pheno)\n",
    "    # For every tissues read rds and extract the bhat and sbhat.\n",
    "    genos = tibble( i = 1:n)\n",
    "    genos = genos%>%mutate(bhat = map(i, ~readRDS(molecular_pheno[[.x,2]])$bhat%>%as.data.frame%>%rownames_to_column),\n",
    "                           sbhat = map(i, ~readRDS(molecular_pheno[[.x,2]])$sbhat%>%as.data.frame%>%rownames_to_column))\n",
    "\n",
    "                      \n",
    "    # Join first two tissues\n",
    "    genos_join_bhat = full_join((genos%>%pull(bhat))[[1]],(genos%>%pull(bhat))[[2]],by = \"rowname\")\n",
    "    genos_join_sbhat = full_join((genos%>%pull(sbhat))[[1]],(genos%>%pull(bhat))[[2]],by = \"rowname\")\n",
    "    \n",
    "    # If there are more tissues, join the rest\n",
    "    if(n > 2){\n",
    "    for(j in 3:n){\n",
    "    genos_join_bhat = full_join(genos_join_bhat,(genos%>%pull(bhat))[[j]],by = \"rowname\")%>%select(-rowname)%>%as.matrix\n",
    "    genos_join_sbhat = full_join(genos_join_sbhat,(genos%>%pull(bhat))[[j]],by = \"rowname\")%>%select(-rowname)%>%as.matrix\n",
    "    }\n",
    "    }\n",
    "    \n",
    "    # Create a list to store the two output\n",
    "    dat = list()\n",
    "    dat$sumstats$bhat = genos_join_bhat\n",
    "    dat$sumstats$sbhat = genos_join_sbhat\n",
    "\n",
    "    # save the rds file\n",
    "    dat%>%saveRDS(\"$[_output]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Performed MV susie\n",
    "This step take the merged files from the previous step to performed mv susies. Before MV susie are done, the X are filter, mean-imputed, and then scaled. The Y are scaled. The covariance matrix of Y are computed via flashier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: [mv_susie_2,mv_susie_cv_2]: command not found\n",
      "bash: input:: command not found\n",
      "bash: output:: command not found\n",
      "bash: f{wd:a}/result/{_input[0]:bn}.transformed_XY.RData,: No such file or directory\n",
      "bash: f{wd:a}/result/{_input[0]:bn}.mv_wgt.txt: No such file or directory\n",
      "bash: task:: command not found\n",
      "bash: R:: command not found\n",
      "bash: syntax error near unexpected token `\"dplyr\"'\n",
      "bash: syntax error near unexpected token `\"tibble\"'\n",
      "bash: syntax error near unexpected token `\"readr\"'\n",
      "bash: syntax error near unexpected token `\"plink2R\"'\n",
      "bash: syntax error near unexpected token `\"mashr\"'\n",
      "bash: syntax error near unexpected token `\"mmbr\"'\n",
      "bash: syntax error near unexpected token `\"flashier\"'\n",
      "bash: syntax error near unexpected token `\"modelr\"'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `min'\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `miss'\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `geno'\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `unique'\n",
      "bash: syntax error near unexpected token `else'\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `rm_col'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `rm_col'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `rm_col'\n",
      "bash: syntax error near unexpected token `mean_impute'\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `miss'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `{'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `covar'\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: tmp: command not found\n",
      "bash: syntax error near unexpected token `{'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `tmp'\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `tmp'\n",
      "bash: syntax error near unexpected token `{'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `}'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: Y_0,: command not found\n",
      "bash: residual_variance: command not found\n",
      "bash: syntax error near unexpected token `)'\n",
      "bash: syntax error near unexpected token `0,ncol'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: m_0: command not found\n",
      "bash: syntax error near unexpected token `m_$[_regions[0]],file'\n",
      "bash: syntax error near unexpected token `('\n",
      "bash: syntax error near unexpected token `scaled_$[_regions[0]],file'\n",
      "bash: syntax error near unexpected token `('\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[mm_prior_2,extract_1]\n",
    "\n",
    "parameter: seed = 999\n",
    "parameter: n_random = 4\n",
    "parameter: n_null = 4\n",
    "\n",
    "input: glob.glob(f'{datadir:a}/*.rds'), group_by = per_chunk\n",
    "output: f\"{wd:a}/cache/{name}_{_index+1}.rds\"\n",
    "\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\"\n",
    "    set.seed(${seed})\n",
    "    matxMax <- function(mtx) {\n",
    "      return(arrayInd(which.max(mtx), dim(mtx)))\n",
    "    }\n",
    "    remove_rownames = function(x) {\n",
    "        for (name in names(x)) rownames(x[[name]]) = NULL\n",
    "        return(x)\n",
    "    }\n",
    "    extract_one_data = function(infile, n_random, n_null) {\n",
    "        # If cannot read the input for some reason then we just skip it, assuming we have other enough data-sets to use.\n",
    "        dat = tryCatch(readRDS(infile)$sumstats, error = function(e) return(NULL))\n",
    "        if (is.null(dat)) return(NULL)\n",
    "        z = abs(dat$bhat/dat$sbhat)\n",
    "        max_idx = matxMax(z)\n",
    "        # strong effect samples\n",
    "        strong = list(bhat = dat$bhat[max_idx[1],,drop=F], sbhat = dat$sbhat[max_idx[1],,drop=F])\n",
    "        # random samples excluding the top one\n",
    "        if (max_idx[1] == 1) {\n",
    "            sample_idx = 2:nrow(z)\n",
    "        } else if (max_idx[1] == nrow(z)) {\n",
    "            sample_idx = 1:(max_idx[1]-1)\n",
    "        } else {\n",
    "            sample_idx = c(1:(max_idx[1]-1), (max_idx[1]+1):nrow(z))\n",
    "        }\n",
    "        random_idx = sample(sample_idx, n_random, replace = T)\n",
    "        random = list(bhat = dat$bhat[random_idx,,drop=F], sbhat = dat$sbhat[random_idx,,drop=F])\n",
    "        # null samples defined as |z| < 2\n",
    "        null.id = which(apply(abs(z), 1, max) < 2)\n",
    "        null_idx = sample(null.id, n_null, replace = F)\n",
    "        null = list(bhat = dat$bhat[null_idx,,drop=F], sbhat = dat$sbhat[null_idx,,drop=F])\n",
    "        return(list(file = infile,random = remove_rownames(random), null = remove_rownames(null), strong = remove_rownames(strong)))\n",
    "    }\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(one_data)) {\n",
    "          return(res)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "              for (s in names(one_data[[d]])) {\n",
    "                  res[[d]][[s]] = rbind(res[[d]][[s]], one_data[[d]][[s]])\n",
    "              }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    res = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      res = merge_data(res, extract_one_data(f, ${n_random}))\n",
    "    }\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mm_prior_3,extract_2]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{wd:a}/output/{name}.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '6G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\"\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "              for (s in names(one_data[[d]])) {\n",
    "                  res[[d]][[s]] = rbind(res[[d]][[s]], one_data[[d]][[s]])\n",
    "              }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    dat = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      dat = merge_data(dat, readRDS(f))\n",
    "    }\n",
    "    # make output consistent in format with \n",
    "    # https://github.com/stephenslab/gtexresults/blob/master/workflows/mashr_flashr_workflow.ipynb\n",
    "    saveRDS(\n",
    "          list(random.z = dat$random$bhat/dat$random$sbhat,\n",
    "           strong.z = dat$strong$bhat/dat$strong$sbhat,\n",
    "           null.z = dat$null$bhat/dat$null$sbhat,\n",
    "           random.b = dat$random$bhat,\n",
    "           strong.b = dat$strong$bhat,\n",
    "           null.b = dat$null$bhat,\n",
    "           null.s = dat$null$sbhat,\n",
    "           random.s = dat$random$sbhat,\n",
    "           strong.s = dat$strong$sbhat,\n",
    "           file = dat$file\n",
    "              ),\n",
    "          ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:2:1: unexpected '['\n1: # Perform FLASH analysis (time estimate: 20min)\n2: [\n   ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:2:1: unexpected '['\n1: # Perform FLASH analysis (time estimate: 20min)\n2: [\n   ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Perform FLASH analysis (time estimate: 20min)\n",
    "[mm_prior_4,flash]\n",
    "# default method for convex optimization\n",
    "parameter: optmethod = \"mixSQP\"\n",
    "parameter: flash_optmethod = \"mixSQP\"\n",
    "# ?Exchangable effect (EE) or exchangable z-scores (EZ)\n",
    "parameter: effect_model = 'EE'\n",
    "output: f\"{wd:a}/output/{name}.EE.flash.rds\"\n",
    "\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container = container\n",
    "    library(flashr)\n",
    "    library(mixsqp)\n",
    "    library(mashr)\n",
    "    my_init_fn <- function(Y, K = 1) {\n",
    "      ret = flashr:::udv_si(Y, K)\n",
    "      pos_sum = sum(ret$v[ret$v > 0])\n",
    "      neg_sum = -sum(ret$v[ret$v < 0])\n",
    "      if (neg_sum > pos_sum) {\n",
    "        return(list(u = -ret$u, d = ret$d, v = -ret$v))\n",
    "      } else\n",
    "      return(ret)\n",
    "    }\n",
    "\n",
    "    flash_pipeline = function(data, ...) {\n",
    "      ## current state-of-the art\n",
    "      ## suggested by Jason Willwerscheid\n",
    "      ## cf: discussion section of\n",
    "      ## https://willwerscheid.github.io/MASHvFLASH/MASHvFLASHnn2.html\n",
    "      ebnm_fn = \"ebnm_ash\"\n",
    "      ebnm_param = list(l = list(mixcompdist = \"normal\",\n",
    "                               optmethod = \"${flash_optmethod}\"),\n",
    "                        f = list(mixcompdist = \"+uniform\",\n",
    "                               optmethod = \"${flash_optmethod}\"))\n",
    "      ##\n",
    "      fl_g <- flashr:::flash_greedy_workhorse(data,\n",
    "                    var_type = \"constant\",\n",
    "                    ebnm_fn = ebnm_fn,\n",
    "                    ebnm_param = ebnm_param,\n",
    "                    init_fn = \"my_init_fn\",\n",
    "                    stopping_rule = \"factors\",\n",
    "                    tol = 1e-3,\n",
    "                    verbose_output = \"odF\")\n",
    "      fl_b <- flashr:::flash_backfit_workhorse(data,\n",
    "                    f_init = fl_g,\n",
    "                    var_type = \"constant\",\n",
    "                    ebnm_fn = ebnm_fn,\n",
    "                    ebnm_param = ebnm_param,\n",
    "                    stopping_rule = \"factors\",\n",
    "                    tol = 1e-3,\n",
    "                    verbose_output = \"odF\")\n",
    "      return(fl_b)\n",
    "    }\n",
    "\n",
    "    cov_flash = function(data, subset = NULL, non_singleton = FALSE, save_model = NULL) {\n",
    "      if(is.null(subset)) subset = 1:mashr:::n_effects(data)\n",
    "      b.center = apply(data$Bhat[subset,], 2, function(x) x - mean(x))\n",
    "      ## Only keep factors with at least two values greater than 1 / sqrt(n)\n",
    "      find_nonunique_effects <- function(fl) {\n",
    "        thresh <- 1/sqrt(ncol(fl$fitted_values))\n",
    "        vals_above_avg <- colSums(fl$ldf$f > thresh)\n",
    "        nonuniq_effects <- which(vals_above_avg > 1)\n",
    "        return(fl$ldf$f[, nonuniq_effects, drop = FALSE])\n",
    "      }\n",
    "\n",
    "      fmodel = flash_pipeline(b.center)\n",
    "      if (non_singleton)\n",
    "          flash_f = find_nonunique_effects(fmodel)\n",
    "      else \n",
    "          flash_f = fmodel$ldf$f\n",
    "      ## row.names(flash_f) = colnames(b)\n",
    "      if (!is.null(save_model)) saveRDS(list(model=fmodel, factors=flash_f), save_model)\n",
    "      if(ncol(flash_f) == 0){\n",
    "        U.flash = list(\"tFLASH\" = t(fmodel$fitted_values) %*% fmodel$fitted_values / nrow(fmodel$fitted_values))\n",
    "      } else{\n",
    "        U.flash = c(cov_from_factors(t(as.matrix(flash_f)), \"FLASH\"),\n",
    "                    list(\"tFLASH\" = t(fmodel$fitted_values) %*% fmodel$fitted_values / nrow(fmodel$fitted_values)))\n",
    "      }\n",
    "      return(U.flash)\n",
    "    }\n",
    "    ##\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mash_set_data(dat$strong.b, dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = cov_flash(dat, non_singleton = TRUE, save_model = \"${_output:n}.model.rds\")\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute data-driven / canonical prior matrices (time estimate: 2h ~ 12h for ~30 49 by 49 matrix mixture)\n",
    "[mm_prior_5,mash_ed_1, mash_teem_1, udr_ed_1]\n",
    "parameter: npc = 3\n",
    "# default method for convex optimization\n",
    "parameter: optmethod = \"mixSQP\"\n",
    "parameter: flash_optmethod = \"mixSQP\"\n",
    "# ?Exchangable effect (EE) or exchangable z-scores (EZ)\n",
    "parameter: effect_model = 'EE'\n",
    "input: output_from('mm_prior_3'), output_from('flash')\n",
    "output: f\"{wd:a}/output/{name}.FL_PC{npc}.rds\"\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '8G', cores = 4, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\",container = container\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    vhat = estimate_null_correlation_simple(mash_set_data(dat$random.b, Shat=dat$random.s, zero_Bhat_Shat_reset = 1E3))\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=0, V=vhat, zero_Bhat_Shat_reset = 1E3)\n",
    "    # FLASH matrices\n",
    "    U.flash = readRDS(${_input[1]:r})\n",
    "    # SVD matrices\n",
    "    U.pca = ${\"cov_pca(mash_data, %s)\" % npc if npc > 0 else \"list()\"}\n",
    "    # Emperical cov matrix\n",
    "    X.center = apply(mash_data$Bhat, 2, function(x) x - mean(x))\n",
    "    Ulist = c(U.flash, U.pca, list(\"XX\" = t(X.center) %*% X.center / nrow(X.center)))\n",
    "    saveRDS(list(mash_data = mash_data, Ulist = Ulist), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mm_prior_6,mash_ed_2]\n",
    "output: f\"{_input:n}.ED.rds\"\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '8G', cores = 14, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\",container = container\n",
    "    dat = readRDS(${_input:r})\n",
    "    # Denoised data-driven matrices\n",
    "    res = mashr:::bovy_wrapper(dat$mash_data, dat$Ulist, logfile=${_output:nr}, tol = 1e-06)\n",
    "    # format to input for simulation with DSC (current pipeline)\n",
    "    saveRDS(list(U=res$Ulist, w=res$pi, loglik=scan(\"${_output:nn}.ED_loglike.log\")), ${_output:r}) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
