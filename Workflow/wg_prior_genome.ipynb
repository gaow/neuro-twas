{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Whole genome prior computation\n",
    "\n",
    "This notebook implements a TWAS analysis workflow using multivariate susie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview (TBD)\n",
    "\n",
    "__Objective__: \n",
    "    To Compute the association between expression and SNP in prepare for TWAS analysis via genotype and multiple molecular phenotype\n",
    "\n",
    "__Background__:\n",
    "    SNP can modulate the functional phenotypes both directly and by modulating the expression levels of genes. \n",
    "Therefore, the integration of expression measurements and a larger scale GWAS summary association statistics will help identify the genes associated with the targeted complex traits. \n",
    "\n",
    "__Significance__:\n",
    "    By applying this method, new candidate genes whose expression level is significantly associated with complex traits can be used in prediction without actually going through the expensive gene expression measurement process. As a relatively small set of gene expression and genotyping, data can be used to impute the expression for a much larger set of phenotyped individuals from their SNP genotype data. \n",
    "\n",
    "__Method__:\n",
    "    The imputed expression can then be viewed as a linear model of genotypes with _weights based on the correlation between SNPs and gene expression__ in the training data while accounting for linkage disequilibrium (LD) SNPs. We then correlated the imputed gene expression to the trait to perform a transcriptome-wide association study (TWAS) and identify significant expression-trait associations. \n",
    " \n",
    "The weights are computed via multivariate susies, the accuracy of such weights are computed via by default 100 times five fold cross validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "We provide a container image `docker://gaow/twas` that contains all software needed to run the pipeline. If you would like to configure it by yourself, please make sure you install the following software before running this notebook:\n",
    "- tidyverse\n",
    "- PLINK\n",
    "- R package mashr\n",
    "- R package mmbr\n",
    "- Output from the following univatiate analysis pipeline: twas_fusion_susie.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Input and Output(TBD)\n",
    "## Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Command interface (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Working example \n",
    "A minimal working example (MWE) dataset that can be downloaded from the following link, which required access:https://drive.google.com/drive/u/0/folders/1N3PbH9hfv5eAikGHI58sXjVsssFkekWj\n",
    "\n",
    "To test the command, please download the mwe folder and run the first among the following command within the mwe/data folder.\n",
    "\n",
    "The time it take to run this MWE shall be around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:3:7: unexpected symbol\n2: \n3: nohup sos\n         ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:3:7: unexpected symbol\n2: \n3: nohup sos\n         ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Test the pipeline with MWE\n",
    "\n",
    "\n",
    "nohup sos run /home/hs3163/GIT/neuro-twas/Workflow/wg_prior_genome.ipynb mm_prior \\\n",
    "--molecular_pheno_dir ./molc_dir/    \\\n",
    "--rds_list ./rds_list_mwe  \\\n",
    "--wd   ./ \\\n",
    "--name \"geneTpmResidualsAgeGenderAdj_rename\" \\\n",
    "--constraint \"Both\" \\\n",
    "--container /mnt/mfs/statgen/containers/twas_latest.sif &\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Actual pipeline running.\n",
    "\n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/wg_prior_genome.ipynb mm_prior \\\n",
    "--molecular_pheno_dir /home/hs3163/Project/Genome_prior/data/molc_dir    \\\n",
    "--rds_list /home/hs3163/Project/Genome_prior/data/rds_list_test  \\\n",
    "--wd   /home/hs3163/Project/Genome_prior/test \\\n",
    "--name \"geneTpmResidualsAgeGenderAdj_rename\" \\\n",
    "--container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "--constraint \"Both\" -s build &\n",
    "\n",
    "\n",
    "\n",
    "## Actual pipeline running.\n",
    "\n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/wg_prior_genome.ipynb flash \\\n",
    "--molecular_pheno_dir /home/hs3163/Project/Genome_prior/data/molc_dir    \\\n",
    "--rds_list /home/hs3163/Project/Genome_prior/data/rds_list  \\\n",
    "--wd   /home/hs3163/Project/Genome_prior/merge \\\n",
    "--name \"geneTpmResidualsAgeGenderAdj_rename\" \\\n",
    "--container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "--constraint \"Both\" \\\n",
    "-J 10 -q csg -c /mnt/mfs/statgen/pbs_template/csg.yml -s build &\n",
    "\n",
    "\n",
    "## Actual pipeline running.\n",
    "\n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/wg_prior_genome.ipynb flash \\\n",
    "--molecular_pheno_dir /home/hs3163/Project/Genome_prior/data/molc_dir    \\\n",
    "--rds_list /home/hs3163/Project/Genome_prior/data/rds_list  \\\n",
    "--wd   /home/hs3163/Project/Genome_prior/merge \\\n",
    "--name \"geneTpmResidualsAgeGenderAdj_rename\" \\\n",
    "--container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "--constraint \"Both\" \\\n",
    "-s build &\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Global parameter settings\n",
    "The section outlined the parameters that can be set in the command interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to a list of folders in which the rds are to be analysised\n",
    "parameter: molecular_pheno_dir = path\n",
    "\n",
    "# Path to a file that lists all the rds to be combined and analysis\n",
    "parameter: rds_list = path\n",
    "# Path to the work directory of this pipeline,where the output will be stored.\n",
    "parameter: wd = path\n",
    "# Specify the number of jobs per run.\n",
    "parameter: job_size = 2\n",
    "# Container option for software to run the analysis: docker or singularity\n",
    "parameter: container = 'gaow/twas'\n",
    "\n",
    "# Input data directory \n",
    "parameter: datadir = path('{wd:a}/input')\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path('{wd:a}/output')\n",
    "# The filename prefix for output data\n",
    "parameter: name = str\n",
    "# handle N = per_chunk data-set in one job\n",
    "parameter: per_chunk = 1000\n",
    "\n",
    "\n",
    "# Whether run flash with constriant or not,default both, ('Constraint', 'No_Constraint' , 'Both')\n",
    "parameter: constraint = 'Both'\n",
    "\n",
    "\n",
    "import glob\n",
    "# Get rds of interest to focus on.\n",
    "regions = [x.strip().split() for x in open(rds_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "molecular_pheno = [x.strip().split() for x in open(molecular_pheno_dir).readlines() if x.strip() and not x.strip().startswith('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "kernel": "SoS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mm_prior_1,merge_1]\n",
    "input:  molecular_pheno_dir, for_each = \"regions\"\n",
    "output: f'{wd:a}/input/{_regions[0]}'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'  \n",
    "\n",
    "R: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"readr\")\n",
    "    molecular_pheno = read_delim(\"$[molecular_pheno_dir]\",delim = \"\\t\")\n",
    "    molecular_pheno = molecular_pheno%>%mutate(dir = map_chr(`#molc_pheno`,~paste(c(`.x`,\"$[_regions[0]]\"),collapse = \"\")))\n",
    "    n = nrow(molecular_pheno)\n",
    "    # For every condition read rds and extract the bhat and sbhat.\n",
    "    genos = tibble( i = 1:n)\n",
    "    genos = genos%>%mutate(bhat = map(i, ~readRDS(molecular_pheno[[.x,2]])$bhat%>%as.data.frame%>%rownames_to_column),\n",
    "                           sbhat = map(i, ~readRDS(molecular_pheno[[.x,2]])$sbhat%>%as.data.frame%>%rownames_to_column))\n",
    "\n",
    "                      \n",
    "    # Join first two conditions\n",
    "    genos_join_bhat = full_join((genos%>%pull(bhat))[[1]],(genos%>%pull(bhat))[[2]],by = \"rowname\")\n",
    "    genos_join_sbhat = full_join((genos%>%pull(sbhat))[[1]],(genos%>%pull(sbhat))[[2]],by = \"rowname\")\n",
    "    \n",
    "    # If there are more conditions, join the rest\n",
    "    if(n > 2){\n",
    "    for(j in 3:n){\n",
    "    genos_join_bhat = full_join(genos_join_bhat,(genos%>%pull(bhat))[[j]],by = \"rowname\")%>%select(-rowname)%>%as.matrix\n",
    "    genos_join_sbhat = full_join(genos_join_sbhat,(genos%>%pull(sbhat))[[j]],by = \"rowname\")%>%select(-rowname)%>%as.matrix\n",
    "    }\n",
    "    }\n",
    "    sumstats = list(bhat=genos_join_bhat, sbhat=genos_join_sbhat)\n",
    "    # save the rds file\n",
    "    saveRDS(file = \"$[_output]\", list(sumstats = sumstats))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mm_prior_2,extract_1]\n",
    "\n",
    "parameter: seed = 999\n",
    "parameter: n_random = 4\n",
    "parameter: n_null = 4\n",
    "\n",
    "input: glob.glob(f'{wd:a}/input/*.rds'), group_by = per_chunk\n",
    "output: f'{wd:a}/cache/{name}_{_index+1}.rds'\n",
    "\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\"\n",
    "    set.seed(${seed})\n",
    "    matxMax <- function(mtx) {\n",
    "      return(arrayInd(which.max(mtx), dim(mtx)))\n",
    "    }\n",
    "    remove_rownames = function(x) {\n",
    "        for (name in names(x)) rownames(x[[name]]) = NULL\n",
    "        return(x)\n",
    "    }\n",
    "    extract_one_data = function(infile, n_random, n_null) {\n",
    "        # If cannot read the input for some reason then we just skip it, assuming we have enough other data-sets to use.\n",
    "        dat = tryCatch(readRDS(infile)$sumstats, error = function(e) return(NULL))\n",
    "        if (is.null(dat)) return(NULL)\n",
    "        z = abs(dat$bhat/dat$sbhat)\n",
    "        max_idx = matxMax(z)\n",
    "        # strong effect samples\n",
    "        strong = list(bhat = dat$bhat[max_idx[1],,drop=F], sbhat = dat$sbhat[max_idx[1],,drop=F])\n",
    "        # random samples excluding the top one\n",
    "        if (max_idx[1] == 1) {\n",
    "            sample_idx = 2:nrow(z)\n",
    "        } else if (max_idx[1] == nrow(z)) {\n",
    "            sample_idx = 1:(max_idx[1]-1)\n",
    "        } else {\n",
    "            sample_idx = c(1:(max_idx[1]-1), (max_idx[1]+1):nrow(z))\n",
    "        }\n",
    "        random_idx = sample(sample_idx, n_random, replace = T)\n",
    "        random = list(bhat = dat$bhat[random_idx,,drop=F], sbhat = dat$sbhat[random_idx,,drop=F])\n",
    "        # null samples defined as |z| < 2\n",
    "        null.id = which(apply(abs(z), 1, max) < 2)\n",
    "        null_idx = sample(null.id, n_null, replace = F)\n",
    "        null = list(bhat = dat$bhat[null_idx,,drop=F], sbhat = dat$sbhat[null_idx,,drop=F])\n",
    "        return(list(file = infile,random = remove_rownames(random), null = remove_rownames(null), strong = remove_rownames(strong)))\n",
    "    }\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(one_data)) {\n",
    "          return(res)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "              for (s in names(one_data[[d]])) {\n",
    "                  res[[d]][[s]] = rbind(res[[d]][[s]], one_data[[d]][[s]])\n",
    "              }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    res = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      res = merge_data(res, extract_one_data(f, ${n_random}))\n",
    "    }\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mm_prior_3,extract_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{wd:a}/output/{name}.rds'\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '6G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\"\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "              for (s in names(one_data[[d]])) {\n",
    "                  res[[d]][[s]] = rbind(res[[d]][[s]], one_data[[d]][[s]])\n",
    "              }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    dat = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      dat = merge_data(dat, readRDS(f))\n",
    "    }\n",
    "    # make output consistent in format with \n",
    "    # https://github.com/stephenslab/gtexresults/blob/master/workflows/mashr_flashr_workflow.ipynb\n",
    "    saveRDS(\n",
    "          list(random.z = dat$random$bhat/dat$random$sbhat,\n",
    "           strong.z = dat$strong$bhat/dat$strong$sbhat,\n",
    "           null.z = dat$null$bhat/dat$null$sbhat,\n",
    "           random.b = dat$random$bhat,\n",
    "           strong.b = dat$strong$bhat,\n",
    "           null.b = dat$null$bhat,\n",
    "           null.s = dat$null$sbhat,\n",
    "           random.s = dat$random$sbhat,\n",
    "           strong.s = dat$strong$sbhat,\n",
    "           file = c(${_input:r,})\n",
    "              ),\n",
    "          ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform FLASH analysis (time estimate: 20min)\n",
    "[mm_prior_4,flash]\n",
    "# default method for convex optimization\n",
    "parameter: optmethod = \"mixSQP\"\n",
    "parameter: flash_optmethod = \"mixSQP\"\n",
    "\n",
    "input: f'{wd:a}/output/{name}.rds'\n",
    "output: f'{wd:a}/output/{name}.{constraint}.flash.rds'\n",
    "\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container = container\n",
    "    library(flashr)\n",
    "    library(mixsqp)\n",
    "    library(mashr)\n",
    "    my_init_fn <- function(Y, K = 1) {\n",
    "      ret = flashr:::udv_si(Y, K)\n",
    "      pos_sum = sum(ret$v[ret$v > 0])\n",
    "      neg_sum = -sum(ret$v[ret$v < 0])\n",
    "      if (neg_sum > pos_sum) {\n",
    "        return(list(u = -ret$u, d = ret$d, v = -ret$v))\n",
    "      } else\n",
    "      return(ret)\n",
    "    }\n",
    "\n",
    "    flash_pipeline = function(data,init_fn = \"my_init_fn\", ...) {\n",
    "      ## Non-negative FLASH suggested by Jason Willwerscheid for when the non-negative factor assumption is reasonable\n",
    "      ## cf: discussion section of\n",
    "      ## https://willwerscheid.github.io/MASHvFLASH/MASHvFLASHnn2.html\n",
    "      ebnm_fn = \"ebnm_ash\"\n",
    "      ebnm_param = list(l = list(mixcompdist = \"normal\",\n",
    "                               optmethod = \"${flash_optmethod}\"),\n",
    "                        f = list(mixcompdist = \"+uniform\",\n",
    "                               optmethod = \"${flash_optmethod}\"))\n",
    "      ##\n",
    "      fl_g <- flashr:::flash_greedy_workhorse(data,\n",
    "                    var_type = \"constant\",\n",
    "                    ebnm_fn = ebnm_fn,\n",
    "                    ebnm_param = ebnm_param,\n",
    "                    init_fn = init_fn,\n",
    "                    stopping_rule = \"factors\",\n",
    "                    tol = 1e-3,\n",
    "                    verbose_output = \"odF\")\n",
    "      fl_b <- flashr:::flash_backfit_workhorse(data,\n",
    "                    f_init = fl_g,\n",
    "                    var_type = \"constant\",\n",
    "                    ebnm_fn = ebnm_fn,\n",
    "                    ebnm_param = ebnm_param,\n",
    "                    stopping_rule = \"factors\",\n",
    "                    tol = 1e-3,\n",
    "                    verbose_output = \"odF\")\n",
    "      return(fl_b)\n",
    "    }\n",
    "\n",
    "    r1cov=function(x){x %*% t(x)}\n",
    "    cov_from_factors = function(f, name){\n",
    "       Ulist = list()\n",
    "       for(i in 1:nrow(f)){\n",
    "         Ulist = c(Ulist,list(r1cov(f[i,])))\n",
    "       }\n",
    "       names(Ulist) = paste0(name,\"_\",(1:nrow(f)))\n",
    "       return(Ulist)\n",
    "     }\n",
    "\n",
    "    ## HS, is this function consistant with the ez model? \n",
    "    cov_flash = function(data, subset = NULL, non_singleton = FALSE, save_model = NULL) {\n",
    "      #if(is.null(subset)) subset = 1:mashr:::n_effects(data)\n",
    "      #b.center = apply(data$Bhat[subset,], 2, function(x) x - mean(x))\n",
    "      ## Only keep factors with at least two values greater than 1 / sqrt(n)\n",
    "    b.center = data$Bhat\n",
    "    find_nonunique_effects <- function(fl) {\n",
    "        thresh <- 1/sqrt(ncol(fl$fitted_values))\n",
    "        vals_above_avg <- colSums(fl$ldf$f > thresh)\n",
    "        nonuniq_effects <- which(vals_above_avg > 1)\n",
    "        return(fl$ldf$f[, nonuniq_effects, drop = FALSE])\n",
    "      }\n",
    "\n",
    "      fmodel = flash_pipeline(b.center)\n",
    "      if (non_singleton)\n",
    "          flash_f = find_nonunique_effects(fmodel)\n",
    "      else \n",
    "          flash_f = fmodel$ldf$f\n",
    "      ## row.names(flash_f) = colnames(b)\n",
    "      if (!is.null(save_model)) saveRDS(list(model=fmodel, factors=flash_f), save_model)\n",
    "      if(ncol(flash_f) == 0){\n",
    "        U.flash = list(\"tFLASH\" = t(fmodel$fitted_values) %*% fmodel$fitted_values / nrow(fmodel$fitted_values))\n",
    "      } else{\n",
    "        U.flash = c(cov_from_factors(t(as.matrix(flash_f)), \"FLASH\"),\n",
    "                    list(\"tFLASH\" = t(fmodel$fitted_values) %*% fmodel$fitted_values / nrow(fmodel$fitted_values)))\n",
    "      }\n",
    "      return(U.flash)\n",
    "    }\n",
    "    ##\n",
    "    dat = readRDS(\"${_input}\")\n",
    "    if(\"${constraint}\" == \"Both\" |\"${constraint}\" == \"No_Constraint\" ){\n",
    "    f.d = flash_set_data(as.matrix(dat$strong.z))\n",
    "    #ycenter = apply(f.d$Y, 2, function(x) x - mean(x))\n",
    "    #f.d$Y = ycenter\n",
    "    f = flash_pipeline(f.d,init_fn = \"udv_si\")\n",
    "    }              \n",
    "    if(\"${constraint}\" == \"Both\"){\n",
    "    dat = mash_set_data(dat$strong.b, dat$strong.s, alpha= 1, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = cov_flash(dat, non_singleton = TRUE, save_model = \"${_output}.model.rds\")\n",
    "    res = c(res,mashr:::cov_from_factors(t(as.matrix(f$ldf$f)), \"FLASH_NC\"),\n",
    "                list(\"tFLASH_NC\" = t(f$fitted_values) %*% f$fitted_values / nrow(f$fitted_values)))                                                     \n",
    "    }else if ( \"${constraint}\" == \"Constraint\" ){\n",
    "    dat = mash_set_data(dat$strong.b, dat$strong.s, alpha= 1, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = cov_flash(dat, non_singleton = TRUE, save_model = \"${_output}.model.rds\")\n",
    "    }else{\n",
    "    res = c(mashr:::cov_from_factors(t(as.matrix(f$ldf$f)), \"FLASH_NC\"),\n",
    "                list(\"tFLASH_NC\" = t(f$fitted_values) %*% f$fitted_values / nrow(f$fitted_values))) \n",
    "    }                                                      \n",
    "    saveRDS(res, \"${_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mm_prior_5,mash_ed_1, udr_ed_1,teem_ed_1,ed_1]\n",
    "parameter: npc = 3\n",
    "input:  f'{wd:a}/output/{name}.rds', f'{wd:a}/output/{name}.{constraint}.flash.rds'\n",
    "output: f'{_input[1]:n}.FL_PC{npc}.rds'\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '8G', cores = 4, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\",container = container\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    V = cor(dat$null.z)\n",
    "    X = dat$strong.z\n",
    "    X[is.na(X)] = 0\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=1, V=V, zero_Bhat_Shat_reset = 1E3)\n",
    "    # FLASH matrices\n",
    "    U.flash = readRDS(${_input[1]:r})\n",
    "    # SVD matrices\n",
    "    U.pca = ${\"cov_pca(mash_data, %s)\" % npc if npc > 0 else \"list()\"}\n",
    "    # Emperical cov matrix\n",
    "    X.center = apply(X, 2, function(x) x - mean(x))\n",
    "    Ulist = c(U.flash, U.pca, list(\"XX\" = t(X.center) %*% X.center / nrow(X.center)))\n",
    "    saveRDS(list(mash_data = mash_data, Ulist = Ulist,X = X, V = V), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mash_ed_2]\n",
    "input:  output_from('mash_ed_1')\n",
    "output: f\"{_input:n}.ED.rds\"\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '8G', cores = 14, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\",container = container\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input:r})\n",
    "    # Denoised data-driven matrices\n",
    "    res = mashr:::bovy_wrapper(dat$mash_data, dat$Ulist, logfile=${_output:nr}, tol = 1e-06)\n",
    "    # format to input for simulation with DSC (current pipeline)\n",
    "    saveRDS(list(U=res$Ulist, w=res$pi, loglik=scan(\"${_output:nn}.ED_loglike.log\")), ${_output:r}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[udr_ed_2]\n",
    "input:  output_from('udr_ed_1')\n",
    "output: f\"{_input:n}.UD_ED.rds\"\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '8G', cores = 14, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\",container = container\n",
    "    library(udr) # udr commit 5265079 with changes to set lower bound on the eigenvalues\n",
    "    dat = readRDS(${_input:r})\n",
    "    # Denoised data-driven matrices\n",
    "    f0 = ud_init(X = dat$X, V = dat$V, U_scaled = list(), U_unconstrained = dat$Ulist, n_rank1=0)\n",
    "    res = ud_fit(f0, control = list(unconstrained.update = \"ed\", resid.update = 'none', maxiter=5000),\n",
    "    verbose=FALSE)\n",
    "    # format to input for simulation with DSC (current pipeline)\n",
    "    saveRDS(list(U=res$U, w=res$w, loglik=res$loglik), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mm_prior_6,ed_2]\n",
    "method = [\"ed\",\"teem\"]\n",
    "input: for_each = \"method\"\n",
    "output: f\"{_input:n}.{_method}.UD_ED.rds\"\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '8G', cores = 14, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\",container = container\n",
    "    library(udr) # udr commit 5265079 with changes to set lower bound on the eigenvalues\n",
    "    dat = readRDS(${_input:r})\n",
    "    # Denoised data-driven matrices\n",
    "    f0 = ud_init(X = dat$X, V = dat$V, U_scaled = list(), U_unconstrained = dat$Ulist, n_rank1=0)\n",
    "    res = ud_fit(f0, control = list(unconstrained.update = \"${_method}\", resid.update = 'none', maxiter=5000),\n",
    "    verbose=FALSE)\n",
    "    # format to input for simulation with DSC (current pipeline)\n",
    "    saveRDS(list(U=res$U, w=res$w, loglik=res$loglik), ${_output:r})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
