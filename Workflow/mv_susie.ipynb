{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# TWAS multivariate susie\n",
    "\n",
    "This notebook implements a TWAS analysis workflow using multivariate susie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview (TBD)\n",
    "\n",
    "__Objective__: \n",
    "    To Compute the association between expression and SNP for TWAS analysis.\n",
    "\n",
    "__Background__:\n",
    "    SNP can modulate the functional phenotypes both directly and by modulating the expression levels of genes. \n",
    "Therefore, the integration of expression measurements and a larger scale GWAS summary association statistics will help identify the genes associated with the targeted complex traits. \n",
    "\n",
    "__Significance__:\n",
    "    By applying this method, new candidate genes whose expression level is significantly associated with complex traits can be used in prediction without actually going through the expensive gene expression measurement process. As a relatively small set of gene expression and genotyping, data can be used to impute the expression for a much larger set of phenotyped individuals from their SNP genotype data. \n",
    "\n",
    "__Method__:\n",
    "    The imputed expression can then be viewed as a linear model of genotypes with _weights based on the correlation between SNPs and gene expression__ in the training data while accounting for linkage disequilibrium (LD) SNPs. We then correlated the imputed gene expression to the trait to perform a transcriptome-wide association study (TWAS) and identify significant expression-trait associations. \n",
    " \n",
    "The weights are computed via various models: blup, bslmm, lasso,top1, and enet. BLUP(best linear unbiased predictors)/bslmm(Bayesian linear mixed model) are conducted using gemma, lasso using plink, and enet (elastic net) using cv.glmnet function in R.\n",
    "\n",
    "Before the weight calculation, the heritability of each gene are computed using GCTA; genes with insignificant heritability were screened out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "We provide a container image `docker://gaow/twas` that contains all software needed to run the pipeline. If you would like to configure it by yourself, please make sure you install the following software before running this notebook:\n",
    "- PLINK\n",
    "- R package mashr\n",
    "- R package mmbr\n",
    "- Output from the univatiate analysis pipeline: twas_fusion_susie.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Input and Output(TBD)\n",
    "## Input\n",
    "- `--gwas_sumstat` The GWAS sum stat text file documenting the associations between the SNP and the disease. It shall contain at least four column: the SNP rsID, the effect allele, the other allele, and the Z-score describing the relationship between the SNP and the disease. \n",
    "- `--genotype_list` An index text file with two columns of chromosome and the corresponding PLINK bed file.\n",
    "- `--molecular-pheno`, The text file containing the table describing the molecular phenotype. It shall have regions(genes) as rows and samples as columnes\n",
    "- `--region_list` The text file with 4 columns specifying the #Chr, P0 (Start position), P1(End position) and names of regions to analyze. The name of the column is not important but the order of the columns. It is also important that the column name of the first column starts with a #. The region_list can can be generated by using another sos pipeline SOS_ROSMAP_gene_exp_processing.ipynb.\n",
    "- `--window` the region span from the specify start and end site for the cis-gene. If the gene expression only have one position column, set the window to a large number like 5E5.\n",
    "- `--window` the region span from the specify start and end site for the cis-gene. If the gene expression only have one position column, set the window to a large number like 5E5.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**FIXME: here are also some text i dug up later in your workflow codes. Please explain them up-front. We would not expect users to read this notebook beyong the \"Working example \" section**\n",
    "This probably is served as a self reminder of what each file do and therefore not expected the user to see, so maybe move them back?\n",
    "\n",
    "```\n",
    "1. The gene expression pheno type, a three column table for each genes, with the first two columns specifing the family ID and within family ID of the samples. In the current case where all samples are unrelated, the first two columns are simply sample ID. The third column is the actual gene expression value.\n",
    "2. The plink trio file for each specific genes, containing only the snps corresponding th the regions whose expression are recorded. In particular, the snp are filtered according to the genetics regions outlined by Position+/-windows.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Output\n",
    "\n",
    "- `.wgt.Rdat` The actual weight data that are computed.\n",
    "- `.weight_list.txt` The index text file recording information about each of the wgt.Rdat data, including the filename, corresponding region ID, Chromosomes, start and end position, heritability, and the SE and P-value of the heritability.\n",
    "- `.dat` the actual TWAS association of the genes, a detailed description of this output are outlined here:http://gusevlab.org/projects/fusion/\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Command interface (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "!sos run twas_fusion.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Working example (TBD)\n",
    "A minimal working example (MWE) dataset that can be downloaded from the private repo:\n",
    "https://github.com/cumc/neuro-twas/blob/master/TWAS_pipeline_MWE%202.zip\n",
    "the genotypes file can be downloaded from the following link:\n",
    "https://data.broadinstitute.org/alkesgroup/FUSION/LDREF.tar.bz2\n",
    "\n",
    "**FIXME: please upload the data to synapse.org and take it out of github. On github we don't store large datasets. Please ask me about account information for synapse.org**\n",
    "\n",
    "The time it take to run this MWE shall be around 2 minutes. Pay extra attention to the gene_start and gene_end position  when using following command on gene_exp file that are not this MWE. Also, when there is too few or too many genes that passed the heritability check, consider increasing or decreasing the --window options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR\u001b[0m: \u001b[91mFailed to locate twas_fusion.ipynb.sos\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test pipeline with test data\n",
    "## Switch back to abosolute path, otherwise there will be file not found error in step 5\n",
    "sos dryrun /Users/haosun/Documents/WG_Reasearch_Assisstant/GIT/freshcopy/neuro-twas/Workflow/mv_susie.ipynb mv_susie \\\n",
    "  --molecular_pheno_dir \"molecular_phenotype_list\" \\\n",
    "  --region_list region_list \\\n",
    "  --wd ./ \\\n",
    "  --name_prefix \"geneTpmResidualsAgeGenderAdj_rename\" \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Global parameter settings\n",
    "The section outlined the parameters that can be set in the command interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to a list of molecular phenotypes that are to be analysised, shall contains a cache file within it.\n",
    "parameter: molecular_pheno_dir = path\n",
    "\n",
    "parameter: region_list = path\n",
    "# Path to the work directory of this pipeline,where the output will be stored.\n",
    "parameter: wd = path\n",
    "# Path to store the output folder\n",
    "parameter: output_path = f'{wd:a}/result'\n",
    "# Specify the number of jobs per run.\n",
    "parameter: job_size = 2\n",
    "# Container option for software to run the analysis: docker or singularity\n",
    "parameter: container = 'gaow/twas'\n",
    "# List of regions that are shared upon all three diretory\n",
    "parameter: region_list = path\n",
    "# name prefix of the molecular_pheno\n",
    "parameter: name_prefix = \"chr\"\n",
    "# Whether impute the missing values\n",
    "parameter: impute = \"TRUE\"\n",
    "# propotion of samples set into testing, set to zero if no cv are needed.\n",
    "parameter: testing_prop = 0.2\n",
    "# Number of training & testing samples used\n",
    "parameter: cv_times = 100\n",
    "\n",
    "\n",
    "# Get regions of interest to focus on.\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "molecular_pheno = [x.strip().split() for x in open(molecular_pheno_dir).readlines() if x.strip() and not x.strip().startswith('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Merge of X (plink) and Y (R)\n",
    "Creat merge list, and then merged based on merged list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "\n",
    "input:  molecular_pheno_dir,for_each = \"regions\"\n",
    "output: f'{molecular_pheno[0]}/cache/{name_prefix}.{_regions[0]}',\n",
    "        f'{molecular_pheno[1]}/cache/{name_prefix}.{_regions[0]}',\n",
    "        f'{molecular_pheno[2]}/cache/{name_prefix}.{_regions[0]}'\n",
    "bash: expand = \"$[ ]\"\n",
    "     echo $[_output[0]]\n",
    "     echo $[_output[1]]\n",
    "     echo $[_output[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mv_susie_1,mv_susie_cv_1]\n",
    "input:  molecular_pheno_dir, for_each = \"regions\"\n",
    "output: f'{wd:a}/cache/{name_prefix}.{_regions[0]}.merged_list',\n",
    "        f'{wd:a}/cache/{name_prefix}.{_regions[0]}.merged.bed',\n",
    "        f'{wd:a}/cache/{name_prefix}.{_regions[0]}.merged.exp'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'  \n",
    "\n",
    "R: expand = \"$[ ]\", stderr = f'{_output[2]}.stderr', stdout = f'{_output[2]}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"plink2R\")\n",
    "    library(\"purrr\")\n",
    "    library(\"readr\")\n",
    "    molecular_pheno = read_delim(\"$[molecular_pheno_dir]\",delim = \"\\t\")\n",
    "    molecular_pheno = molecular_pheno%>%mutate(dir = map_chr(`#molc_pheno`,~paste(c(`.x`,\"/cache/$[name_prefix].$[_regions[0]]\"),collapse = \"\")))\n",
    "    n = nrow(molecular_pheno)\n",
    "    # For every tissues read plink, and extract the fam df.\n",
    "    genos = tibble( i = 1:n)\n",
    "    genos = genos%>%mutate(fam = map(i, ~read_plink(molecular_pheno[[.x,2]])$fam%>%as_tibble()%>%mutate(name = paste(V1,\":\",V2,sep = \"\"))%>%select(name,V6)))\n",
    "    \n",
    "    # Join two tissues\n",
    "    genos_join_phe_$[_regions[0]] = full_join((genos%>%pull(fam))[[1]],(genos%>%pull(fam))[[2]],by = \"name\")\n",
    "    \n",
    "    # If there are more tissues, join the rest\n",
    "    if(n > 2){\n",
    "    for(j in 3:n){\n",
    "    genos_join_phe_$[_regions[0]] = full_join(genos_join_phe_$[_regions[0]],(genos%>%pull(fam))[[j]],by = \"name\")\n",
    "    }\n",
    "    }\n",
    "    genos_join_phe_$[_regions[0]]%>%readr::write_delim(\"$[_output[2]]\",delim = \"\\t\")\n",
    "    \n",
    "    # Create merge list\n",
    "    molecular_pheno[2]%>%readr::write_delim(\"$[_output[0]]\",delim = \"\\t\",col_names=FALSE)\n",
    "\n",
    "\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output[1]}.stderr', stdout = f'{_output[1]}.stdout',container = container\n",
    "\n",
    "    # create the merged output X\n",
    "    plink --bfile '$[next(iter(molecular_pheno[0]))]/cache/$[name_prefix].$[_regions[0]]'\\\n",
    "          --merge-list $[_output[0]] \\\n",
    "          --mac 1 \\\n",
    "          --make-bed \\\n",
    "          --out $[_output[1]:n] \\\n",
    "          --allow-no-sex\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## performed MV susie\n",
    "This step take the merged files from the previous step to performed susies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mv_susie_2,mv_susie_cv_2]\n",
    "input: group_by = 3, group_with = 'regions'\n",
    "output:  f'{wd:a}/result/{_input[0]:bn}.mv_susie.model.RData',\n",
    "         f'{wd:a}/result/{_input[0]:bn}.transformed_XY.RData',\n",
    "         f'{wd:a}/result/{_input[0]:bn}.mv_wgt.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand= \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"readr\")\n",
    "    library(\"plink2R\")\n",
    "    library(\"mashr\")\n",
    "    library(\"mmbr\")  \n",
    "    library(\"flashier\")\n",
    "    library(\"modelr\")\n",
    "    # Define functions\n",
    "    ###Functions to compute MAF and missing genotype rate\n",
    "    compute_maf <- function(geno){\n",
    "      f <- mean(geno,na.rm = TRUE)/2\n",
    "      return(min(f, 1-f))\n",
    "    }\n",
    "    \n",
    "    compute_missing <- function(geno){\n",
    "      miss <- sum(is.na(geno))/length(geno)\n",
    "      return(miss)\n",
    "    }\n",
    "    \n",
    "    mean_impute <- function(geno){\n",
    "      f <- apply(geno, 2, function(x) mean(x,na.rm = TRUE))\n",
    "      for (i in 1:length(f)) geno[,i][which(is.na(geno[,i]))] <- f[i]\n",
    "      return(geno)\n",
    "    }\n",
    "    \n",
    "    is_zero_variance <- function(x) {\n",
    "      if (length(unique(x))==1) return(T)\n",
    "      else return(F)\n",
    "    }\n",
    "    ### Filter X matrix\n",
    "    filter_X <- function(X, missing_rate_thresh, maf_thresh) {\n",
    "      rm_col <- which(apply(X, 2, compute_missing) > missing_rate_thresh)\n",
    "      if (length(rm_col)) X <- X[, -rm_col]\n",
    "      rm_col <- which(apply(X, 2, compute_maf) < maf_thresh)\n",
    "      if (length(rm_col)) X <- X[, -rm_col]\n",
    "      rm_col <- which(apply(X, 2, is_zero_variance))\n",
    "      if (length(rm_col)) X <- X[, -rm_col]\n",
    "      return(mean_impute(X))\n",
    "    }\n",
    "    ###Function to calculate the covariance matrix of Y via flash\n",
    "    compute_cov_flash <- function(Y, miss=NULL){\n",
    "      if(is.null(miss)){\n",
    "        fl <- flashier::flash(Y, var.type = 2, prior.family = c(flashier::prior.normal(), flashier::prior.normal.scale.mix()), backfit = TRUE, verbose.lvl=0)\n",
    "      } else {\n",
    "        fl <- flashier::flash(Y[-miss, ], var.type = 2, prior.family = c(flashier::prior.normal(), flashier::prior.normal.scale.mix()), backfit = TRUE, verbose.lvl=0)\n",
    "      }  \n",
    "      if(fl$n.factors==0){\n",
    "        covar <- diag(fl$residuals.sd^2)\n",
    "      } else {\n",
    "        fsd <- sapply(fl$fitted.g[[1]], '[[', \"sd\")\n",
    "        covar <- diag(fl$residuals.sd^2) + crossprod(t(fl$flash.fit$EF[[2]]) * fsd)\n",
    "      }\n",
    "      return(covar)\n",
    "    }\n",
    "    ###Function to impute the missing X with means and then scale and center X\n",
    "      impute_and_transform = function(genos,impute = TRUE){\n",
    "      tmp = genos\n",
    "      if(impute == TRUE){\n",
    "      for(i in 1:ncol(tmp)){\n",
    "        tmp[,i]=coalesce(tmp[,i],mean(tmp[,i]%>%na.omit()))%>%scale()}\n",
    "        return(tmp)\n",
    "      } else {\n",
    "    for(i in 1:ncol(tmp)){\n",
    "        tmp[,i]=tmp[,i]%>%scale()}\n",
    "        return(tmp)}}\n",
    "    if ($[impute] == TRUE){\n",
    "    # Load X data\n",
    "    X_$[_regions[0]]_raw = read_plink(\"$[_input[1]:n]\")$bed\n",
    "    # Filter X by 0.1 NA and 0.01 MAF\n",
    "    X_$[_regions[0]]_ftr = filter_X(X_$[_regions[0]]_raw,0.1,0.01)\n",
    "    X_$[_regions[0]] = impute_and_transform(X_$[_regions[0]]_ftr)\n",
    "    # Load Y data\n",
    "    Y_$[_regions[0]] = read_delim(\"$[_input[2]]\",delim = \"\\t\")\n",
    "    # Reorder Y based on X\n",
    "    Y_$[_regions[0]] = Y_$[_regions[0]]%>%arrange(match(name,rownames(X_$[_regions[0]])))%>%select(-name)%>%as.matrix()\n",
    "    # Compute the Cov matrix for Y via flashier\n",
    "    Y_$[_regions[0]]_cov = Y_$[_regions[0]]%>%compute_cov_flash()\n",
    "    # Impute the missing Y by mean and scale it.\n",
    "    # Get prior\n",
    "    prior_covar <- create_mash_prior(sample_data = list(X=X_$[_regions[0]],Y=Y_$[_regions[0]], residual_variance= Y_$[_regions[0]]_cov, max_mixture_len=-1))\n",
    "    } else {\n",
    "    # Load data\n",
    "    Y_$[_regions[0]] = read_delim(\"$[_input[2]]\",delim = \"\\t\")\n",
    "    # Remove NA from bed\n",
    "    X_$[_regions[0]] = read_plink(\"$[_input[1]:n]\")$bed%>%t()%>%na.omit()%>%t()%>%impute_and_transform(impute = FALSE)\n",
    "    # Reorder Y based on X\n",
    "    Y_$[_regions[0]] = Y_$[_regions[0]]%>%arrange(match(name,rownames(X_$[_regions[0]])))%>%select(-name)%>%as.matrix()\n",
    "    Y_complete_$[_regions[0]] = Y_$[_regions[0]]%>%na.omit()\n",
    "    # Get prior\n",
    "    # Compute the Cov matrix for Y_complete\n",
    "    Y_$[_regions[0]]_cov = cov(Y_complete_$[_regions[0]])}\n",
    "   \n",
    "    prior_covar <- create_mash_prior(sample_data = list(X=X_$[_regions[0]],Y=Y_$[_regions[0]], residual_variance= Y_$[_regions[0]]_cov, max_mixture_len=-1))\n",
    "    m_$[_regions[0]] = msusie(X_$[_regions[0]], \n",
    "                Y_$[_regions[0]], \n",
    "                L=10, \n",
    "                prior_variance=prior_covar,\n",
    "                residual_variance = Y_$[_regions[0]]_cov,\n",
    "                precompute_covariances = TRUE)\n",
    " \n",
    "    #Add a hsq sub for the msusie object\n",
    "    hsq_$[_regions[0]]=rep(0,ncol(Y_$[_regions[0]]))\n",
    "    for (i in 1:ncol(Y_$[_regions[0]])){\n",
    "      hsq_$[_regions[0]][i] = var(predict(m_$[_regions[0]])[,i])/var(Y_$[_regions[0]][,i]%>%na.omit())}\n",
    "    m_$[_regions[0]]$hsq = hsq_$[_regions[0]]\n",
    "        \n",
    "    #Output: model with hsq estimated\n",
    "    save(m_$[_regions[0]],file = \"$[_output[0]]\")\n",
    "    #Output: scaled data\n",
    "    scaled_$[_regions[0]] = list(X_$[_regions[0]],Y_$[_regions[0]])\n",
    "    save(scaled_$[_regions[0]],file = \"$[_output[1]]\")\n",
    "    #Output: Weight\n",
    "    m_$[_regions[0]]$coef%>%as.data.frame()%>%write_delim(\"$[_output[2]]\",delim = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Perform Crossvalidation and stored the relevent matrixs\n",
    "This step load the scaled X,Y output from the previouse step, perform CV, and calculate the diagnosis paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mv_susie_cv_3]\n",
    "input: group_by = 3, group_with = 'regions'\n",
    "output:  f'{wd:a}/result/{_input[0]:bn}.mv_susie.model.cv.RData'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand= \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"readr\")\n",
    "    library(\"plink2R\")\n",
    "    library(\"mashr\")\n",
    "    library(\"mmbr\")  \n",
    "    library(\"flashier\")\n",
    "    library(\"purrr\")\n",
    "    library(\"modelr\")\n",
    "    \n",
    "    # Define functions\n",
    "    compute_cov_flash <- function(Y, miss=NULL){\n",
    "      if(is.null(miss)){\n",
    "        fl <- flashier::flash(Y, var.type = 2, prior.family = c(flashier::prior.normal(), flashier::prior.normal.scale.mix()), backfit = TRUE, verbose.lvl=0)\n",
    "      } else {\n",
    "        fl <- flashier::flash(Y[-miss, ], var.type = 2, prior.family = c(flashier::prior.normal(), flashier::prior.normal.scale.mix()), backfit = TRUE, verbose.lvl=0)\n",
    "      }  \n",
    "      if(fl$n.factors==0){\n",
    "        covar <- diag(fl$residuals.sd^2)\n",
    "      } else {\n",
    "        fsd <- sapply(fl$fitted.g[[1]], '[[', \"sd\")\n",
    "        covar <- diag(fl$residuals.sd^2) + crossprod(t(fl$flash.fit$EF[[2]]) * fsd)\n",
    "      }\n",
    "      return(covar)\n",
    "    }\n",
    "    \n",
    "    ## Compute rmse function\n",
    "    compute_rmse = function(raw,fitted){\n",
    "    rmse = rep(0,ncol(raw))\n",
    "    for (i in 1:ncol(raw)){\n",
    "      rmse[i] = ((fitted - raw)[,i])^2%>%mean(na.rm = TRUE)%>%sqrt() \n",
    "      }\n",
    "    return(rmse)\n",
    "    }\n",
    "    \n",
    "    ## Compute r2 function\n",
    "    ## Compute r2 function\n",
    "    compute_r2 = function(raw,fitted){\n",
    "      r2 = rep(0,ncol(raw))\n",
    "      for (j in 1:ncol(raw)){\n",
    "        r2[j] =  cor(fitted[,j][which(!is.na(raw[,j]))],raw[,j]%>%na.omit())^2\n",
    "      }\n",
    "      return(r2)\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    # Load data\n",
    "    full_model = attach('$[_input[0]]')\n",
    "    full_model = full_model$m_$[_regions[0]]\n",
    "    X = attach('$[_input[1]]')$scaled_$[_regions[0]][[1]]\n",
    "    Y = attach('$[_input[1]]')$scaled_$[_regions[0]][[2]]\n",
    "    \n",
    "    # Merged the X and Y for producing testing and training set for modelr cv\n",
    "    cv_df_raw = cbind(X,Y)%>%as_tibble() \n",
    "    cv_df = crossv_mc(cv_df_raw, 3,test = 0.2)%>%mutate(\n",
    "      train_X = map(train,~as_tibble(.x)[1:ncol(X)]%>%as.matrix),\n",
    "      train_Y = map(train,~as_tibble(.x)[(ncol(X)+1):(ncol(X)+ncol(Y))]%>%as.matrix),\n",
    "      test_X = map(test,~as_tibble(.x)[1:ncol(X)]%>%as.matrix),\n",
    "      test_Y = map(test,~as_tibble(.x)[(ncol(X)+1):(ncol(X)+ncol(Y))]%>%as.matrix)\n",
    "    )\n",
    "                           \n",
    "    # Compute the cov matrix for training set Y based on the choice of imputation                  \n",
    "                         \n",
    "    if ($[impute] == TRUE){\n",
    "        cv_df = cv_df%>%mutate( \n",
    "        cov = map(train_Y,~.x%>%compute_cov_flash())\n",
    "      )\n",
    "        }else{\n",
    "      cv_df = cv_df%>%mutate(\n",
    "        cov = map(train_Y,~cov(.x%>%na.omit)))}\n",
    "        \n",
    "    # Actual cv\n",
    "    \n",
    "    cv_df = cv_df%>%mutate(\n",
    "    \n",
    "    ## Get the prior\n",
    "    \n",
    "      prior = pmap(list(train_X,train_Y,cov),function(first,second,third)(\n",
    "        create_mash_prior(sample_data = list(X=first,Y=second, residual_variance= third, max_mixture_len=-1,center=F,scale=F))\n",
    "        )),\n",
    "        \n",
    "    ## Do msusie\n",
    "    \n",
    "      msusie = pmap(list(train_X,train_Y,cov,prior),function(first,second,third,forth)(\n",
    "        msusie(first,second, L=10, prior_variance=forth,residual_variance = third,precompute_covariances = TRUE)\n",
    "        )))\n",
    "    \n",
    "    # Extract data \n",
    "    \n",
    "    cv_df = cv_df%>%mutate(\n",
    "      weight = map(msusie,~.x$coef),\n",
    "      test_fitted = map2(msusie,test_X,~predict.mmbr(.x,.y)),\n",
    "      rmse = map2(test_Y,test_fitted,~compute_rmse(.x,.y)),\n",
    "      r2 = map2(test_Y,test_fitted,~compute_r2(.x,.y))\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    \n",
    "    mean_rmse = cv_df%>%pull(rmse)%>%as.data.frame()%>%t()%>%as_tibble()%>%colMeans()\n",
    "    mean_r2 = cv_df%>%pull(r2)%>%as.data.frame()%>%t()%>%as_tibble()%>%colMeans()\n",
    "    \n",
    "    # Save metrics\n",
    "    full_model$rmse = mean_rmse\n",
    "    full_model$r2 = mean_r2\n",
    "  \n",
    "    #Output\n",
    "    save(full_model,file = \"$[_output[0]]\")\n",
    "    \n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Merging all the RData file\n",
    "THis step merged the output from  the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mv_susie_3]\n",
    "input: group_by = \"all\"\n",
    "output:  f'{wd:a}/mv.RData'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h',  mem = '6G', tags = f'{step_name}_{_output:bn}'\n",
    "R: expand= \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"readr\")\n",
    "    library(\"purrr\")\n",
    "    # Load a template\n",
    "    region = read_delim(\"$[region_list]\",delim =\"\\t\")%>%select(ID = `#region` )\n",
    "    # get the path\n",
    "    dir = \"$[_input[0]:d]/\"\n",
    "    pre = \"$[name_prefix]\"\n",
    "    sur = \".mv_susie.model.RData\"\n",
    "    region = region%>%mutate(path = map(ID, ~paste(collapse = \"\", c(dir,pre,\".\",.x,sur))))\n",
    "    # Load the data\n",
    "    output = region%>%mutate(env = map(path,~attach(.x)),\n",
    "                            tb_name = map_chr(ID,~paste(collapse = \"_\", c(\"m\",.x))),\n",
    "                             model = map2(env,tb_name , ~get(.y,env = .x)))\n",
    "    # Save the combined output\n",
    "    save(output,file = \"$[_output]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mv_susie_cv_4]\n",
    "input: group_by = \"all\"\n",
    "output:  f'{wd:a}/mv_cv.RData'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h',  mem = '6G', tags = f'{step_name}_{_output:bn}'\n",
    "R: expand= \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"readr\")\n",
    "    library(\"purrr\")\n",
    "    # Load a template\n",
    "    region = read_delim(\"$[region_list]\",delim =\"\\t\")%>%select(ID = `#region` )\n",
    "    # get the path\n",
    "    dir = \"$[_input[0]:d]/\"\n",
    "    pre = \"$[name_prefix]\"\n",
    "    sur = \".mv_susie.model.cv.RData\"\n",
    "    region = region%>%mutate(path = map(ID, ~paste(collapse = \"\", c(dir,pre,\".\",.x,sur))))\n",
    "    # Load the data\n",
    "    output = region%>%mutate(env = map(path,~attach(.x)),\n",
    "                            tb_name = map_chr(ID,~paste(collapse = \"_\", c(\"m\",.x))),\n",
    "                             model = map2(env,tb_name , ~get(.y,env = .x)))\n",
    "    # Save the combined output\n",
    "    save(output,file = \"$[_output]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
