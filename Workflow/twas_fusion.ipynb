{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# TWAS FUSION analysis\n",
    "\n",
    "This notebook implements a TWAS analysis workflow using the FUSION software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim\n",
    "\n",
    "The aim of this pipeline is to build a prediction model for gene expression using cis-SNPs by obtaining \"weights\" from multiple regression of gene expression levels on SNP genotypes. The weights can then be combined with the GWAS data to test for association between predicted gene expression and GWAS phenotypes. Either GWAS summary statistics or GWAS genotype + phenotype data can be used for the association testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "To Compute association between expression and SNP for TWAS analysis.\n",
    "SNP can modulate the functional phenotypes both directly and by modulating the expression levels of genes. \n",
    "Therefore, the integration of expression measurements and a larger scale GWAS summary association statistics will be desirable to identify the genes associated with the targeted complex traits. \n",
    "\n",
    "By the application of this method, new candidates genes whose expression level is significantly associated to complex traits can be used in prediction without actually going through the expensive gene expression measurement process. As a relatively small set of gene expression and genotyping data can be used to impute the expression for a much larger set of phenotyped individuals from their SNP genotype data. \n",
    "\n",
    "The imputed expression can then be viewed as a linear model of genotypes with __weights based on the correlation between SNPs and gene expression__ in the training data while accounting for linkage disequilibrium (LD) among SNPs. We then correlated the imputed gene expression to the trait to perform a transcriptome-wide association study (TWAS) and identify __significant expression-trait associations__. \n",
    " \n",
    "The weights are computed via variouse models: blup, bslmm,lasso,top1 and enet. blup(best linear unbiased predictors)/bslmm(Bayesian linear mixed model) are conducted using gemma, lasso using plink,and enet(elastic net) using \tcv.glmnet function in R.\n",
    "\n",
    "Before the weight calculation, heritability of each genes are computed using GCTA, genes with insignificant heritability were screened out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "We provide a container image `docker://gaow/twas` that contains all software needed to run the pipeline. If you would like to configure it by yourself, please make sure you install the following software before running this notebook:\n",
    "- GCTA\n",
    "- PLINK\n",
    "- GEMMA\n",
    "- Modified `Fusion.compute_weights.R` scripts can be found [in this github repo](https://github.com/cumc/neuro-twas/blob/master/Workflow/FUSION.compute_weights.R).\n",
    "- The original `FUSION.assoc_test.R` script can be found [in the author's github repo](https://github.com/gusevlab/fusion_twas).\n",
    "\n",
    "You need to make both `Fusion.compute_weights.R` and `FUSION.assoc_test.R` executable with `Rscript` command. See [this line]() for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Input and Output\n",
    "## Input\n",
    "- `--gene_exp_file`, including a gene expression table with gene name as rows and sample as column. Each gene also required at least one column specifing the chr and pos(or alternatively Start and End position), the chr column shall have the same formation as how the chromosome are specified in the genotype file. The sample names shall be the same as the sample ID in the genotype file. \n",
    "- `--geno-path`, the path of a genotype inventory, which lists the path of all genotype file in bgen format or in plink format.\n",
    "- `--genotype_file_directory`, path to the genotype inventory, list the path of all genotype file in plink format.\n",
    "- `--genotype_prefix`, the prefix of the genotype file, up to the chromosome name.\n",
    "- `--window` the region span from the specify start and end site for the cis-gene. If the gene expression only have one position column, set the window to a large number like 5E5.\n",
    "\n",
    "**FIXME: Hao, text below are moved from your analysis code. This information should belong to input data section**\n",
    "\n",
    "```\n",
    "# PATH TO DIRECTORY CONTAINING genotype_file_directory DATA (FROM FUSION WEBSITE or https://data.broadinstitute.org/alkesgroup/FUSION/genotype_file_directory.tar.bz2)\n",
    "# THIS IS USED TO RESTRICT INPUT SNPS TO REFERENCE IDS ONLY\n",
    "# GEUVADIS DATA WAS DOWNLOADED FROM https://www.ebi.ac.uk/arrayexpress/experiments/E-GEUV-1/files/analysis_results/\n",
    "```\n",
    "\n",
    "**FIXME: here are also some text i dug up later in your workflow codes. Please explain them up-front. We would not expect users to read this notebook beyong the \"Working example \" section**\n",
    "\n",
    "```\n",
    "1. The gene expression pheno type, a three column table for each genes, with the first two columns specifing the family ID and within family ID of the samples. In the current case where all samples are unrelated, the first two columns are simply sample ID. The third column is the actual gene expression value.\n",
    "2. The plink trio file for each specific genes, containing only the snps corresponding th the regions whose expression are recorded. In particular, the snp are filtered according to the genetics regions outlined by Position+/-windows.\n",
    "```\n",
    "\n",
    "Also I changed your genotype file input to a `genotype_list`: An index text file with two columns of chromosome and the corresponding PLINK bed file.\n",
    "\n",
    "## Output\n",
    "\n",
    "-- .wgt.Rdat The actualy weight data that are computed\n",
    "\n",
    "-- .hsq the file containing the heritibality information for the genes\n",
    "\n",
    "-- .All_passed_gene.hsq the file that containing the heritibality information for all the genes in this run\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Command interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR\u001b[0m: \u001b[91mNotebook JSON is invalid: %s\u001b[0m\n",
      "usage: sos run twas_fusion.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  STEP\n",
      "  AssocTest\n",
      "\n",
      "Global Workflow Options:\n",
      "  --GCTA VAL (as path, required)\n",
      "                        MAKE SURE FUSION.compute_weights.R IS IN YOUR PATH FILL\n",
      "                        IN THESE PATHS For mac user, the mac version of GCTA\n",
      "                        shall be downloaded saperately, the one came with the\n",
      "                        Fusion package will not work.\n",
      "  --PLINK VAL (as path, required)\n",
      "  --GEMMA VAL (as path, required)\n",
      "  --compute-weight-rscp VAL (as path, required)\n",
      "                        Required the customized fusion.compute_weight.mod.R\n",
      "                        script, other wise will not work\n",
      "  --assoc-test-rscp VAL (as path, required)\n",
      "                        the R script from fusion to conduct association test is\n",
      "                        required\n",
      "  --gene-exp-file VAL (as path, required)\n",
      "                        Path to the input data,must include the name of the file\n",
      "                        itself (Phenotype data,)\n",
      "  --sumstat-file VAL (as path, required)\n",
      "                        (sumstats data: the GWAS result between SNP and disease)\n",
      "  --wd VAL (as path, required)\n",
      "                        PATH TO where the weight shall be stored or stored.\n",
      "  --assoc-test-result  f'{wd}/result'\n",
      "\n",
      "                        Path to where the output of association testing shall be\n",
      "                        stored, by default it is the result subdiretory of the\n",
      "                        wd .\n",
      "  --genotype-file-directory VAL (as path, required)\n",
      "                        PATH TO DIRECTORY CONTAINING genotype_file_directory\n",
      "                        DATA (FROM FUSION WEBSITE or https://data.broadinstitute\n",
      "                        .org/alkesgroup/FUSION/genotype_file_directory.tar.bz2)\n",
      "  --genotype-prefix VAL (as path, required)\n",
      "                        SUBSAMPLE THESE TO THE genotype_file_directory SNPS FOR\n",
      "                        EFFICIENCY\n",
      "  --chrom 3 (as int)\n",
      "                        Specify the column in the genexpression file that\n",
      "                        contains chromosome\n",
      "  --gene-start VAL (as int, required)\n",
      "                        If both the start and end region are specified, then\n",
      "                        their column can be specified saperately\n",
      "  --gene-end VAL (as int, required)\n",
      "  --gene-name 1 (as int)\n",
      "                        Specify the column in the genexpression file that\n",
      "                        contains the name of the gene\n",
      "  --window 50000 (as int)\n",
      "                        Specify the scanning window for the gene position, set\n",
      "                        default to 50000 if start = end\n",
      "  --model 'blsmm,blup,lasso,top1,enet'\n",
      "                        Specify what model are used to compute weights, do not\n",
      "                        used blsmm during testing, the runtime is way too long\n",
      "\n",
      "Sections\n",
      "  STEP_1:               Make the Paitient_ID File\n",
      "  STEP_2:               Preparing the phenotype files\n",
      "  STEP_3:               Preparing the genotype file for each genes\n",
      "  STEP_4:               Actual weight computation\n",
      "  STEP_5:               Create wgt.Rdat map for Assoc_testing\n",
      "  STEP_6, AssocTest:    Association test\n",
      "  STEP_7:               Clean up dummy file\n"
     ]
    }
   ],
   "source": [
    "!sos run twas_fusion.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Working example\n",
    "A minimal working example (MWE) dataset that can be downloaded from the private repo:\n",
    "https://github.com/cumc/neuro-twas/blob/master/TWAS_pipeline_MWE%202.zip\n",
    "the genotypes file can be downloaded from the following link:\n",
    "https://data.broadinstitute.org/alkesgroup/FUSION/LDREF.tar.bz2\n",
    "\n",
    "**FIXME: please upload the data to synapse.org and take it out of github. On github we don't store large datasets. Please ask me about account information for synapse.org**\n",
    "\n",
    "The time it take to run this MWE shall be around 2 minutes. Pay extra attention to the gene_start and gene_end position  when using following command on gene_exp file that are not this MWE. Also, when there is too few or too many genes that passed the heritability check, consider increasing or decreasing the --window options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mcptWgt_1\u001b[0m: \n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=2) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=3) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=1) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=4) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=5) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=6) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=7) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=8) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=10) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=9) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=11) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=12) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=13) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=14) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=15) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=16) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=17) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=18) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=19) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=20) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=21) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=22) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=23) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=24) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=25) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=26) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=27) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=28) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=29) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=30) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=32) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=31) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=33) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m (index=34) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_1\u001b[0m output:   \u001b[32m/Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/cache/gene_exp_file.ENSG00000136237.12.exp /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/cache/gene_exp_file.ENSG00000136237.12.pheno... (68 items in 35 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mcptWgt_2\u001b[0m: \n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=0) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=1) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=2) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=3) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=4) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=5) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=6) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=7) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=8) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=9) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=10) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=12) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=11) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=13) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=14) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=15) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=16) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=18) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=17) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=19) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=21) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=20) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=22) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=23) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=24) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=25) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=26) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=28) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=27) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=29) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=31) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=30) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=32) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m (index=33) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_2\u001b[0m output:   \u001b[32m/Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/cache/gene_exp_file.ENSG00000136237.12.bed /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/cache/gene_exp_file.ENSG00000136237.12.bim... (102 items in 34 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mcptWgt_3\u001b[0m: Actual weight computation\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=3) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=0) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=1) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=2) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=5) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=4) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=6) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=7) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=9) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=10) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=8) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=11) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=13) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=12) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=14) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=17) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=15) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=16) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=18) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=20) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=19) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=22) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=21) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=23) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=26) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=24) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=25) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=30) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=27) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=28) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=31) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=29) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=32) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m (index=33) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_3\u001b[0m output:   \u001b[32m/Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/WEIGHTS/gene_exp_file.ENSG00000136237.12.wgt.RDat /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/WEIGHTS/gene_exp_file.ENSG00000259382.1.wgt.RDat... (34 items in 34 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mcptWgt_4\u001b[0m: \n",
      "INFO: \u001b[32mcptWgt_4\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_4\u001b[0m output:   \u001b[32m/Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/WEIGHTS/gene_exp_file.pos\u001b[0m\n",
      "INFO: Running \u001b[32mcptWgt_5\u001b[0m: Association test\n",
      "INFO: \u001b[32mcptWgt_5\u001b[0m (index=3) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_5\u001b[0m (index=0) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_5\u001b[0m (index=2) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_5\u001b[0m (index=1) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_5\u001b[0m (index=5) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_5\u001b[0m (index=4) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_5\u001b[0m (index=6) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_5\u001b[0m output:   \u001b[32m/Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/result/gene_exp_file_7.dat /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/result/gene_exp_file_3.dat... (7 items in 7 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mcptWgt_6\u001b[0m: Clean up dummy file\n",
      "usage: rm [-f | -i] [-dPRrvW] file ...\n",
      "       unlink file\n",
      "usage: rm [-f | -i] [-dPRrvW] file ...\n",
      "       unlink file\n",
      "usage: rm [-f | -i] [-dPRrvW] file ...\n",
      "       unlink file\n",
      "usage: rm [-f | -i] [-dPRrvW] file ...\n",
      "       unlink file\n",
      "\u001b[95mWARNING\u001b[0m: \u001b[95mCommand '/bin/bash .sos/cptWgt_6_0_3a1d25d4.sh' returned non-zero exit status 64.\u001b[0m\n",
      "INFO: \u001b[32mcptWgt_6\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcptWgt_6\u001b[0m output:   \u001b[32m/Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/error_gene/no_plink.txt /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at/error_gene/no_wgt_computed.txt\u001b[0m\n",
      "INFO: Workflow cptWgt (ID=48ba153ea93d88bc) is executed successfully with 6 completed steps and 112 completed substeps.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test pipeline with test data\n",
    "## Switch back to abosolute path, otherwise there will be file not found error in step 5\n",
    "sos run twas_fusion.ipynb cptWgt \\\n",
    "  --gwas_sumstat /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/data/sum_stat.sumstats \\\n",
    "  --molecular-pheno /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/data/gene_exp_file.txt \\\n",
    "  --wd /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/Working_at \\\n",
    "  --genotype_list /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/data/ld_ref \\\n",
    "  --region_list /Users/haosun/Documents/WG_Reasearch_Assisstant/Fusion/Project/Updated/data/region_list.txt \\\n",
    "  --region_chrom 3 \\\n",
    "  --region_name 1 \\\n",
    "  --region_start 4 \\\n",
    "  --region_end 4 \\\n",
    "  --data_start 5 \\\n",
    "  --window 500000 \\\n",
    "  --model blup lasso top1 enet \\\n",
    "  --container gaow/twas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Association test only \n",
    "If using exisiting weight, use the association test(AssocTest) workflow. A minimum working example is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Global parameter settings\n",
    "The section outlined the parameters that can be set in the command interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to the input molecular phenotype data.\n",
    "parameter: molecular_pheno = path\n",
    "# Path to GWAS summary statistics data (association results between SNP and disease in a GWAS)\n",
    "parameter: gwas_sumstat = path\n",
    "# An index text file with two columns of chromosome and the corresponding PLINK bed file.\n",
    "parameter: genotype_list = path\n",
    "# An index text file with 4 columns specifying the chr, start, end and names of regions to analyze\n",
    "parameter: region_list = path\n",
    "# Path to the work directory of the weight computation: output weights and cache will be saved to this directory.\n",
    "parameter: wd = path('./')\n",
    "# Path to list of weights\n",
    "parameter: weights_list = f'{wd:a}/WEIGHTS/{molecular_pheno:bn}.weights_list.txt'\n",
    "# Specify the output of Association test only result, stored under wd by default\n",
    "parameter: assoc_test_result = f'{wd}/result'\n",
    "# Specify the column in the molecular_pheno file that contains chromosome\n",
    "parameter: region_chrom = int\n",
    "# Specific the column in the molecular_pheno file that contains start position\n",
    "parameter: region_start = int\n",
    "# Specific the column in the molecular_pheno file that contains end position\n",
    "parameter: region_end = int\n",
    "# Specify the column in the molecular_pheno file that contains the name of the region of interest\n",
    "parameter: region_name = int\n",
    "# Specify the column in the molecular_pheno file where the actual data start\n",
    "parameter: data_start = int\n",
    "# Specify the scanning window for the up and downstream radius to analyze around the region of interest, in units of Kb\n",
    "parameter: window = 50000\n",
    "\n",
    "# Specify what model are used to compute weights.\n",
    "# Notice that `blsmm` can be very resource consuming.\n",
    "parameter: model = ['blsmm', 'blup' , 'lasso', 'top1', 'enet']\n",
    "# Container option for software to run the analysis: docker or singularity\n",
    "parameter: container = 'gaow/twas'\n",
    "# Get regions of interest to focus on.\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "geno_inventory = dict([x.strip().split() for x in open(genotype_list).readlines() if x.strip() and not x.strip().startswith('#')])\n",
    "\n",
    "import os\n",
    "def get_genotype_file(chrom, genotype_list, geno_inventory):\n",
    "    chrom = f'{chrom}'\n",
    "    if chrom.startswith('chr'):\n",
    "        chrom = chrom[3:]\n",
    "    if chrom not in geno_inventory:\n",
    "        geno_file = \"error\"\n",
    "    else:\n",
    "        geno_file = geno_inventory[chrom]\n",
    "    if not os.path.isfile(geno_file):\n",
    "        # relative path\n",
    "        if not os.path.isfile(f'{genotype_list:ad}/' + geno_file):\n",
    "            raise ValueError(f\"Cannot find genotype file {geno_file}\")\n",
    "        else:\n",
    "            geno_file = f'{genotype_list:ad}/' + geno_file\n",
    "    geno_file = geno_file.split(\".b\",1)[0]\n",
    "    return f'{geno_file}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## FIXME: add explanation what this step is\n",
    "\n",
    "Comments & FIXME:\n",
    "\n",
    "1. This step should not allow for any errors. In general, we should not bypass errors, but rather face and fix them if they are under our control. I therefore changed `cat` to `stop`. In the future please try to avoid using allow error.\n",
    "2. I added `#` to the header of region_list file and skipped any lines starting with `#`. Please see global section `regions` variable.\n",
    "3. Notice the use of `:a` -- please always use that to make output files absolute paths. Then it would not matter where your work directory is. You only have to do it for the very first step of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[cptWgt_1]\n",
    "\n",
    "input: molecular_pheno, for_each = \"regions\"\n",
    "output: f'{wd:a}/cache/{_input:bn}.{_regions[3]}.exp',\n",
    "        f'{wd:a}/cache/{_input:bn}.{_regions[3]}.pheno'\n",
    "\n",
    "R: expand = \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container\n",
    "    # Get the line number for the region in the file\n",
    "    line_num = system(\"awk '($$[region_name]==\\\"$[_regions[3]]\\\") {print NR}' $[_input]\", intern=T)\n",
    "    if (length(line_num) == 0){\n",
    "      stop( \"Cannot find $[_regions[3]] in column $[region_name]  $[_input]\")}\n",
    "    yi <- data.table::fread(file = $[_input:r], skip = as.integer(line_num) - 1, nrows = 1)\n",
    "    samplenames_yi <- data.table::fread(file = $[_input:r], skip = 0, nrows = 1)\n",
    "    colnames(yi) <- colnames(samplenames_yi)\n",
    "    readr::write_tsv(yi, path = \"$[_output[0]]\", na = \"NA\", append = FALSE, col_names = TRUE, quote_escape = \"double\")\n",
    "    yi <- as.data.frame(yi[, $[data_start]:ncol(yi), drop = FALSE])\n",
    "    yj <- rbind(colnames(yi),colnames(yi),yi)\n",
    "    readr::write_tsv(as.data.frame(t(yj)), path = \"$[_output[1]]\", na = \"NA\", append = FALSE, col_names = TRUE, quote_escape = \"double\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## FIXME: please document it briefly\n",
    "\n",
    "Here we use `|| true` to bypass any errors (it ensures return status of the command is always `0`)\n",
    "\n",
    "Notice the use of `group_with` which removes the need to extract region information from previous output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[cptWgt_2]\n",
    "input: group_by = 2, group_with = 'regions'\n",
    "output: f'{_input[0]:n}.bed',\n",
    "        f'{_input[0]:n}.bim',\n",
    "        f'{_input[0]:n}.fam'\n",
    "# look up for genotype file\n",
    "geno_file = get_genotype_file(_regions[0],genotype_list,geno_inventory)\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container\n",
    "    ##### Get the locus genotypes for $[_regions[3]]\n",
    "    plink --bfile $[geno_file] \\\n",
    "    --pheno $[_input[1]] \\\n",
    "    --make-bed \\\n",
    "    --out $[_output[0]:n] \\\n",
    "    --chr $[_regions[0]] \\\n",
    "    --from-bp $[int(_regions[1]) + window ] \\\n",
    "    --to-bp $[int(_regions[2]) - window ] \\\n",
    "    --keep $[_input[1]] \\\n",
    "    --allow-no-sex || true\n",
    "    touch $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## FIXME: please document it briefly\n",
    "\n",
    "Comment: Notice in the docker file, I ended up using the original `FUSION.compute_weights.R` in the docker file. Your modified version of `FUSION.compute_weights.R` is unnecessary, and has hard-coded path `./output` which is not desiable. The original script in fact works without a problem with my current version of pipeline\n",
    "\n",
    "The take home lesson is that for path issues please always try to fix at pipeline level, not changing the core code for it. You should let pipeline tools take care of path issue because one feature of such tools is management of various paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#Actual weight computation \n",
    "[cptWgt_3]\n",
    "input: group_with = 'regions'\n",
    "output: f'{wd:a}/WEIGHTS/{_input[0]:bn}.wgt.RDat'\n",
    "import os\n",
    "skip_if(os.path.getsize(f'{_input[0]}') == 0)\n",
    "\n",
    "bash: expand= \"$[ ]\",stderr = f'{_output[0]:nn}.stderr', stdout = f'{_output[0]:nn}.stdout', volumes=[f'{wd:a}:{wd:a}'], container = container\n",
    "    FUSION.compute_weights.R \\\n",
    "    --bfile $[_input[0]:n] \\\n",
    "    --tmp $[_input[0]:n].tmp \\\n",
    "    --out $[_output[0]:nn] \\\n",
    "    --verbose 0 \\\n",
    "    --save_hsq \\\n",
    "    --PATH_gcta `which gcta64` \\\n",
    "    --PATH_gemma `which gemma` \\\n",
    "    --models \"$[\",\".join(model)]\"   \n",
    "    ## Creat dummy output file that will be deleted next step\n",
    "    touch $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## FIXME: Please document\n",
    "\n",
    "Comment: it's a lot cleaner to just code with SoS (using python syntax). See example below where I rewrote your R code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[cptWgt_4]\n",
    "input: group_by = \"all\"\n",
    "# Lets not loops over sequential substep, but loop inside one.\n",
    "output: weights_list\n",
    "import os\n",
    "weight_files = [str(x) for x in _input if not os.path.getsize(x) == 0]\n",
    "regions_dict = dict([(x[3], (x[0], x[1], x[2])) for x in regions])\n",
    "res = [[\"WGT\", \"ID\",\"CHR\",\"P0\",\"P1\"]]\n",
    "for item in weight_files:\n",
    "    name = os.path.basename(item).rstrip('wgt.RDat').lstrip(f'.{molecular_pheno:bn}')\n",
    "    res.append([item,name,regions_dict[name][0], regions_dict[name][1], regions_dict[name][2]])\n",
    "with open(_output, 'w') as f:\n",
    "    f.write('\\n'.join(['\\t'.join(x) for x in res]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## FIXME: please document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Association test\n",
    "[cptWgt_5,assocTest]\n",
    "depends: weights_list\n",
    "\n",
    "chrom_list = get_output(f'cut -f 3 {_input} | tail -n+2').strip().split(\"\\n\")\n",
    "chrom_list = list(set(chrom_list))\n",
    "\n",
    "input: gwas_sumstat, for_each = \"chrom_list\"\n",
    "output:f'{assoc_test_result}/{molecular_pheno:bn}_{_chrom_list}.dat'\n",
    "\n",
    "geno_file = get_genotype_file(_chrom_list, genotype_list, geno_inventory)\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "    FUSION.assoc_test.R \\\n",
    "    --sumstats $[_input] \\\n",
    "    --weights $[weights_list] \\\n",
    "    --weights_dir / \\\n",
    "    --ref_ld_chr $[geno_file]. \\\n",
    "    --chr $[_chrom_list] \\\n",
    "    --out $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## FIXME: please document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Clean up dummy file\n",
    "[cptWgt_6]\n",
    "input: group_by = \"all\"\n",
    "output: f'{wd:a}/error_no_plink.log',\n",
    "        f'{wd:a}/error_no_wgt_computed.log'\n",
    "bash: expand= \"$[ ]\"\n",
    "    find $[wd:a]/cache/*.bim -size 0 -print > $[_output[0]]\n",
    "    rm `find $[wd:a]/cache/*.bim -size 0`\n",
    "    rm `find $[wd:a]/cache/*.fam -size 0`\n",
    "    rm `find $[wd:a]/cache/*.bed -size 0`\n",
    "    find $[wd:a]/WEIGHTS/*.wgt.RDat -size 0 -print > $[_output[1]]\n",
    "    rm `find $[wd:a]/WEIGHTS/*.wgt.RDat -size 0`    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.21.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
