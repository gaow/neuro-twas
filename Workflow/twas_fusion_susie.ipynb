{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# TWAS FUSION analysis\n",
    "\n",
    "This notebook implements a TWAS analysis workflow using the FUSION software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim\n",
    "\n",
    "The aim of this pipeline is to build a prediction model for gene expression using cis-SNPs by obtaining \"weights\" from multiple regression of gene expression levels on SNP genotypes. The weights can then be combined with the GWAS data to test for association between predicted gene expression and GWAS phenotypes. Either GWAS summary statistics or GWAS genotype + phenotype data can be used for the association testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "__Objective__: \n",
    "    To Compute the association between expression and SNP for TWAS analysis.\n",
    "\n",
    "__Background__:\n",
    "    SNP can modulate the functional phenotypes both directly and by modulating the expression levels of genes. \n",
    "Therefore, the integration of expression measurements and a larger scale GWAS summary association statistics will help identify the genes associated with the targeted complex traits. \n",
    "\n",
    "__Significance__:\n",
    "    By applying this method, new candidate genes whose expression level is significantly associated with complex traits can be used in prediction without actually going through the expensive gene expression measurement process. As a relatively small set of gene expression and genotyping, data can be used to impute the expression for a much larger set of phenotyped individuals from their SNP genotype data. \n",
    "\n",
    "__Method__:\n",
    "    The imputed expression can then be viewed as a linear model of genotypes with _weights based on the correlation between SNPs and gene expression__ in the training data while accounting for linkage disequilibrium (LD) SNPs. We then correlated the imputed gene expression to the trait to perform a transcriptome-wide association study (TWAS) and identify significant expression-trait associations. \n",
    " \n",
    "The weights are computed via various models: blup, bslmm, lasso,top1, and enet. BLUP(best linear unbiased predictors)/bslmm(Bayesian linear mixed model) are conducted using gemma, lasso using plink, and enet (elastic net) using cv.glmnet function in R.\n",
    "\n",
    "Before the weight calculation, the heritability of each gene are computed using GCTA; genes with insignificant heritability were screened out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "We provide a container image `docker://gaow/twas` that contains all software needed to run the pipeline. If you would like to configure it by yourself, please make sure you install the following software before running this notebook:\n",
    "- GCTA\n",
    "- PLINK\n",
    "- GEMMA\n",
    "- Modified `Fusion.compute_weights.R` scripts can be found [in this github repo](https://github.com/cumc/neuro-twas/blob/master/Workflow/FUSION.compute_weights.R).\n",
    "- The original `FUSION.assoc_test.R` script can be found [in the author's github repo](https://github.com/gusevlab/fusion_twas).\n",
    "\n",
    "You need to make both `Fusion.compute_weights.R` and `FUSION.assoc_test.R` executable with `Rscript` command. See [this line]() for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Input and Output\n",
    "## Input\n",
    "- `--gwas_sumstat` The GWAS sum stat text file documenting the associations between the SNP and the disease. It shall contain at least four column: the SNP rsID, the effect allele, the other allele, and the Z-score describing the relationship between the SNP and the disease. \n",
    "- `--genotype_list` An index text file with two columns of chromosome and the corresponding PLINK bed file.\n",
    "- `--molecular-pheno`, The text file containing the table describing the molecular phenotype. It shall have regions(genes) as rows and samples as columnes\n",
    "- `--region_list` The text file with 4 columns specifying the #Chr, P0 (Start position), P1(End position) and names of regions to analyze. The name of the column is not important but the order of the columns. It is also important that the column name of the first column starts with a #. The region_list can can be generated by using another sos pipeline SOS_ROSMAP_gene_exp_processing.ipynb.\n",
    "- `--window` the region span from the specify start and end site for the cis-gene. If the gene expression only have one position column, set the window to a large number like 5E5.\n",
    "- `--window` the region span from the specify start and end site for the cis-gene. If the gene expression only have one position column, set the window to a large number like 5E5.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**FIXME: here are also some text i dug up later in your workflow codes. Please explain them up-front. We would not expect users to read this notebook beyong the \"Working example \" section**\n",
    "This probably is served as a self reminder of what each file do and therefore not expected the user to see, so maybe move them back?\n",
    "\n",
    "```\n",
    "1. The gene expression pheno type, a three column table for each genes, with the first two columns specifing the family ID and within family ID of the samples. In the current case where all samples are unrelated, the first two columns are simply sample ID. The third column is the actual gene expression value.\n",
    "2. The plink trio file for each specific genes, containing only the snps corresponding th the regions whose expression are recorded. In particular, the snp are filtered according to the genetics regions outlined by Position+/-windows.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Output\n",
    "\n",
    "- `.wgt.Rdat` The actual weight data that are computed.\n",
    "- `.weight_list.txt` The index text file recording information about each of the wgt.Rdat data, including the filename, corresponding region ID, Chromosomes, start and end position, heritability, and the SE and P-value of the heritability.\n",
    "- `.dat` the actual TWAS association of the genes, a detailed description of this output are outlined here:http://gusevlab.org/projects/fusion/\n",
    "- `susie.RData` a R object containing all the susie output for each of the regions\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Command interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run twas_fusion.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  twas_fusion\n",
      "  association_test\n",
      "\n",
      "Global Workflow Options:\n",
      "  --molecular-pheno VAL (as path, required)\n",
      "                        Path to the input molecular phenotype data.\n",
      "  --gwas-sumstat VAL (as path, required)\n",
      "                        Path to GWAS summary statistics data (association\n",
      "                        results between SNP and disease in a GWAS)\n",
      "  --genotype-list VAL (as path, required)\n",
      "                        An index text file with two columns of chromosome and\n",
      "                        the corresponding PLINK bed file.\n",
      "  --region-list VAL (as path, required)\n",
      "                        An index text file with 4 columns specifying the chr,\n",
      "                        start, end and names of regions to analyze\n",
      "  --wd . (as path)\n",
      "                        Path to the work directory of the weight computation:\n",
      "                        output weights and cache will be saved to this\n",
      "                        directory.\n",
      "  --weights-path  f'{wd:a}/WEIGHTS'\n",
      "\n",
      "                        Specify the directory to save fitted weights\n",
      "  --weights-list  f'{weights_path}/{molecular_pheno:bn}.weights_list.txt'\n",
      "\n",
      "                        Path to list of weights\n",
      "  --region-name VAL (as int, required)\n",
      "                        Specify the column in the molecular_pheno file that\n",
      "                        contains the name of the region of interest\n",
      "  --data-start VAL (as int, required)\n",
      "                        Specify the column in the molecular_pheno file where the\n",
      "                        actual data start\n",
      "  --window 50000 (as int)\n",
      "                        Specify the scanning window for the up and downstream\n",
      "                        radius to analyze around the region of interest, in\n",
      "                        units of Kb\n",
      "  --model blsmm blup lasso top1 enet (as list)\n",
      "                        Specify what model are used to compute weights. Notice\n",
      "                        that `blsmm` can be very resource consuming.\n",
      "  --container 'gaow/twas'\n",
      "                        Container option for software to run the analysis:\n",
      "                        docker or singularity\n",
      "\n",
      "Sections\n",
      "  twas_fusion_1:\n",
      "  twas_fusion_2:\n",
      "  twas_fusion_3:        Actual weight computation\n",
      "  twas_fusion_4:\n",
      "  twas_fusion_5, association_test: Association test\n",
      "  twas_fusion_6:        Clean up dummy file\n"
     ]
    }
   ],
   "source": [
    "!sos run twas_fusion.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Working example\n",
    "The MWE file is availble at :\n",
    "\"https://www.synapse.org/#!Synapse:syn24179064/files/\"\n",
    "\n",
    "The time it take to run this MWE shall be around 2 minutes. Pay extra attention to the gene_start and gene_end position  when using following command on gene_exp file that are not this MWE. Also, when there is too few or too many genes that passed the heritability check, consider increasing or decreasing the --window options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR\u001b[0m: \u001b[91mFailed to locate twas_fusion.ipynb.sos\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test pipeline with test data\n",
    "## Switch back to abosolute path, otherwise there will be file not found error in step 5\n",
    "sos run twas_fusion_susie.ipynb susie \\\n",
    "  --molecular-pheno ./molecular_phenotype \\\n",
    "  --wd ./ \\\n",
    "  --genotype_list ./geno_dir\\\n",
    "  --region_list ./region_list \\\n",
    "  --region_name 1 \\\n",
    "  --data_start 5 \\\n",
    "  --window 500000 \\\n",
    "  --model blup lasso top1 enet \\\n",
    "  --container /mnt/mfs/statgen/containers/twas_latest.sif \n",
    " \n",
    "sos run /home/hs3163/GIT/neuro-twas/Workflow/twas_fusion_susie.ipynb compute_wgt \\\n",
    "  --molecular-pheno ./molecular_phenotype \\\n",
    "  --wd ./ \\\n",
    "  --genotype_list ./geno_dir \\\n",
    "  --region_list ./region_list \\\n",
    "  --region_name 1 \\\n",
    "  --data_start 5 \\\n",
    "  --window 500000 \\\n",
    "  --model blup lasso top1 enet \\\n",
    "  --container /mnt/mfs/statgen/containers/twas_latest.sif \n",
    "  \n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/twas_fusion_susie.ipynb association_test  \\\n",
    "  --gwas_sumstat /home/hs3163/Project/Alz/data/AD_sumstats_Jansenetal_2019sept.txt \\\n",
    "  --molecular-pheno  /home/hs3163/Project/Rosmap/data/gene_exp/DLPFC/geneTpmResidualsAgeGenderAdj_rename.txt  \\\n",
    "  --wd /home/hs3163/Project/mwe/ass_test_mwe/AC \\\n",
    "  --genotype_list /home/hs3163/Project/Rosmap/data/Rosmap_wgs_genotype_list.txt \\\n",
    "  --region_list /home/hs3163/Project/mwe/mv_susie_mwe/region_list \\\n",
    "  --region_name 1 \\\n",
    "  --data_start 2 \\\n",
    "  --window 500000 \\\n",
    "  --weights_path ~/Project/mwe/mv_susie_mwe/wgt/AC \\\n",
    "  --weights_list ~/Project/mwe/mv_susie_mwe/wgt/AC/All_wgt_list.txt \\\n",
    "  --container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "  --job_size 1\\\n",
    "  --asso_mem \"2G\" \\\n",
    "  -J 6 -q csg -c /mnt/mfs/statgen/pbs_template/csg.yml \\ \n",
    "  -s build &\n",
    "  \n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/twas_fusion_susie.ipynb twas_fusion  \\\n",
    "  --gwas_sumstat /home/hs3163/Project/Alz/data/AD_sumstats_Jansenetal_2019sept.txt \\\n",
    "  --molecular-pheno  /home/hs3163/Project/Rosmap/data/gene_exp/DLPFC/geneTpmResidualsAgeGenderAdj_rename.txt  \\\n",
    "  --wd /home/hs3163/Project/Alz/Alz_AC_SNP/ \\\n",
    "  --genotype_list /home/hs3163/Project/Rosmap/data/Rosmap_wgs_genotype_list.txt \\\n",
    "  --region_list /home/hs3163/Project/Alz/data/RSM_Alz_region_list_AC.txt \\\n",
    "  --region_name 1 \\\n",
    "  --data_start 2 \\\n",
    "  --window 500000 \\\n",
    "  --weights_path /home/hs3163/Project/Alz/Alz_AC_SNP/WEIGHTS \\\n",
    "  --weights_list /home/hs3163/Project/Alz/Alz_AC_SNP/WEIGHTS/All_wgt_list.txt \\\n",
    "  --container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "  --job_size 1\\\n",
    "  --asso_mem \"20G\" \\\n",
    "  -J 50 -q csg -c /home/hs3163/GIT/neuro-twas/template/csg.yml \\\n",
    "  -s build &\n",
    "  \n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/twas_fusion_susie.ipynb twas_fusion  \\\n",
    "  --gwas_sumstat /home/hs3163/Project/Alz/data/AD_sumstats_Jansenetal_2019sept.txt \\\n",
    "  --molecular-pheno  /home/hs3163/Project/Rosmap/data/gene_exp/DLPFC/geneTpmResidualsAgeGenderAdj_rename.txt  \\\n",
    "  --wd /home/hs3163/Project/Alz/Alz_DLPFC_SNP/ \\\n",
    "  --genotype_list /home/hs3163/Project/Rosmap/data/Rosmap_wgs_genotype_list.txt \\\n",
    "  --region_list /home/hs3163/Project/Alz/data/RSM_Alz_region_list_DLPFC.txt \\\n",
    "  --region_name 1 \\\n",
    "  --data_start 2 \\\n",
    "  --window 500000 \\\n",
    "  --weights_path /home/hs3163/Project/Alz/Alz_DLPFC_SNP/WEIGHTS \\\n",
    "  --weights_list /home/hs3163/Project/Alz/Alz_DLPFC_SNP/WEIGHTS/All_wgt_list.txt \\\n",
    "  --container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "  --job_size 1\\\n",
    "  --asso_mem \"20G\" \\\n",
    "  -J 50 -q csg -c /mnt/mfs/statgen/pbs_template/csg.yml \\\n",
    "  -s build &\n",
    "\n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/twas_fusion_susie.ipynb twas_fusion  \\\n",
    "  --gwas_sumstat /home/hs3163/Project/Alz/data/AD_sumstats_Jansenetal_2019sept.txt \\\n",
    "  --molecular-pheno  /home/hs3163/Project/Rosmap/data/gene_exp/PCC/geneTpmResidualsAgeGenderAdj_rename.txt  \\\n",
    "  --wd /home/hs3163/Project/Alz/Alz_PCC_SNP/ \\\n",
    "  --genotype_list /home/hs3163/Project/Rosmap/data/Rosmap_wgs_genotype_list.txt \\\n",
    "  --region_list /home/hs3163/Project/Alz/data/RSM_Alz_region_list_PCC.txt \\\n",
    "  --region_name 1 \\\n",
    "  --data_start 2 \\\n",
    "  --window 500000 \\\n",
    "  --weights_path /home/hs3163/Project/Alz/Alz_PCC_SNP/WEIGHTS \\\n",
    "  --weights_list /home/hs3163/Project/Alz/Alz_PCC_SNP/WEIGHTS/All_wgt_list.txt \\\n",
    "  --container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "  --job_size 1\\\n",
    "  --asso_mem \"20G\" \\\n",
    "  -J 50 -q csg -c /home/hs3163/GIT/neuro-twas/template/csg.yml \\\n",
    "  -s build &\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Association test only \n",
    "If using exisiting weight, use the association test(AssocTest) workflow. A minimum working example is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyboard Interrupt\n"
     ]
    }
   ],
   "source": [
    "nohup sos run ~/GIT/neuro-twas/Workflow/twas_fusion_susie.ipynb association_test  \\\n",
    "  --gwas_sumstat /home/hs3163/Project/Alz/data/AD_sumstats_Jansenetal_2019sept.txt \\\n",
    "  --molecular-pheno /home/hs3163/Project/Rosmap/data/gene_exp/DLPFC/geneTpmResidualsAgeGenderAdj_rename.txt \\\n",
    "  --wd /home/hs3163/Project/TWAS/wgt/DLPFC/ \\\n",
    "  --genotype_list /home/hs3163/Project/Rosmap/data/Rosmap_wgs_genotype_list.txt \\\n",
    "  --region_list /home/hs3163/Project/mwe/mv_susie_mwe/region_list \\\n",
    "  --region_name 1 \\\n",
    "  --data_start 2 \\\n",
    "  --window 500000 \\\n",
    "  --weights_path /home/hs3163/Project/TWAS/wgt/DLPFC \\\n",
    "  --weights_list /home/hs3163/Project/TWAS/wgt/DLPFC/All_wgt_list.txt \\\n",
    "  --container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "  --job_size 1\\\n",
    "  --asso_mem \"50G\" \\\n",
    "  -J 40 -q csg -c /home/hs3163/GIT/neuro-twas/template/csg.yml \\\n",
    "  -s build &\n",
    "  \n",
    "  \n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/twas_fusion_susie.ipynb association_test  \\\n",
    "  --gwas_sumstat /home/hs3163/Project/Alz/data/AD_sumstats_Jansenetal_2019sept.txt \\\n",
    "  --molecular-pheno /home/hs3163/Project/Rosmap/data/gene_exp/PCC/geneTpmResidualsAgeGenderAdj_rename.txt \\\n",
    "  --wd /home/hs3163/Project/TWAS/wgt/PCC/ \\\n",
    "  --genotype_list /home/hs3163/Project/Rosmap/data/Rosmap_wgs_genotype_list.txt \\\n",
    "  --region_list /home/hs3163/Project/mwe/mv_susie_mwe/region_list \\\n",
    "  --region_name 1 \\\n",
    "  --data_start 2 \\\n",
    "  --window 500000 \\\n",
    "  --weights_path /home/hs3163/Project/TWAS/wgt/PCC \\\n",
    "  --weights_list /home/hs3163/Project/TWAS/wgt/PCC/All_wgt_list.txt \\\n",
    "  --container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "  --job_size 1\\\n",
    "  --asso_mem \"50G\" \\\n",
    "  -J 40 -q neurology -c /home/hs3163/GIT/neuro-twas/template/csg.yml \\\n",
    "  -s build &\n",
    "  \n",
    "nohup sos run ~/GIT/neuro-twas/Workflow/twas_fusion_susie.ipynb association_test  \\\n",
    "  --gwas_sumstat /home/hs3163/Project/Alz/data/AD_sumstats_Jansenetal_2019sept.txt \\\n",
    "  --molecular-pheno /home/hs3163/Project/Rosmap/data/gene_exp/DLPFC/geneTpmResidualsAgeGenderAdj_rename.txt \\\n",
    "  --wd /home/hs3163/Project/TWAS/wgt/AC/ \\\n",
    "  --genotype_list /home/hs3163/Project/Rosmap/data/Rosmap_wgs_genotype_list.txt \\\n",
    "  --region_list /home/hs3163/Project/mwe/mv_susie_mwe/region_list \\\n",
    "  --region_name 1 \\\n",
    "  --data_start 2 \\\n",
    "  --window 500000 \\\n",
    "  --weights_path /home/hs3163/Project/TWAS/wgt/AC \\\n",
    "  --weights_list /home/hs3163/Project/TWAS/wgt/AC/All_wgt_list.txt \\\n",
    "  --container /mnt/mfs/statgen/containers/twas_latest.sif \\\n",
    "  --job_size 1\\\n",
    "  --asso_mem \"50G\" \\\n",
    "  -J 40 -q csg -c /home/hs3163/GIT/neuro-twas/template/csg.yml \\\n",
    "  -s build &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Global parameter settings\n",
    "The section outlined the parameters that can be set in the command interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to the input molecular phenotype data.\n",
    "parameter: molecular_pheno = path\n",
    "# An index text file with two columns of chromosome and the corresponding PLINK bed file.\n",
    "parameter: genotype_list = path\n",
    "# An index text file with 4 columns specifying the chr, start, end and names of regions to analyze\n",
    "parameter: region_list = path\n",
    "# Path to the work directory of the weight computation: output weights and cache will be saved to this directory.\n",
    "parameter: wd = path('./')\n",
    "# Specify the directory to save fitted weights\n",
    "parameter: weights_path = f'{wd:a}/WEIGHTS'\n",
    "# Path to list of weights\n",
    "parameter: weights_list = f'{weights_path}/{molecular_pheno:bn}.weights_list.txt'\n",
    "# Path to store the output folder\n",
    "parameter: output_path = f'{wd:a}/result'\n",
    "# Specify the column in the molecular_pheno file that contains the name of the region of interest\n",
    "parameter: region_name = int\n",
    "# Specify the column in the molecular_pheno file where the actual data start\n",
    "parameter: data_start = int\n",
    "# Specify the scanning window for the up and downstream radius to analyze around the region of interest, in units of Kb\n",
    "parameter: window = 50000\n",
    "# Specify the number of jobs per run.\n",
    "parameter: job_size = 2\n",
    "# Specify the minumum p value for heratibility for the study to continue\n",
    "parameter: hsq_p = 0.01\n",
    "\n",
    "# Specify what model are used to compute weights.\n",
    "# Notice that `blsmm` can be very resource consuming.\n",
    "parameter: model = ['bslmm', 'blup' , 'lasso', 'top1', 'enet']\n",
    "# Container option for software to run the analysis: docker or singularity\n",
    "parameter: container = 'gaow/twas'\n",
    "\n",
    "# Propotion of samples set into testing, set to zero if no cv are needed.\n",
    "parameter: testing_prop = 0.2\n",
    "# Number of training & testing samples used\n",
    "parameter: cv_times = 2\n",
    "\n",
    "# parameters for the susie pipelines.\n",
    "parameter: causal_variables_L = 10\n",
    "parameter: scaled_prior_variance = 0.1\n",
    "\n",
    "# Get regions of interest to focus on.\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "\n",
    "geno_inventory = dict([x.strip().split() for x in open(genotype_list).readlines() if x.strip() and not x.strip().startswith('#')])\n",
    "\n",
    "import os\n",
    "def get_genotype_file(chrom, genotype_list, geno_inventory):\n",
    "    chrom = f'{chrom}'\n",
    "    if chrom.startswith('chr'):\n",
    "        chrom = chrom[3:]\n",
    "    if chrom not in geno_inventory:\n",
    "        geno_file = f'{chrom}'\n",
    "    else:\n",
    "        geno_file = geno_inventory[chrom]\n",
    "    if not os.path.isfile(geno_file):\n",
    "        # relative path\n",
    "        if not os.path.isfile(f'{genotype_list:ad}/' + geno_file):\n",
    "            raise ValueError(f\"Cannot find genotype file {geno_file}\")\n",
    "        else:\n",
    "            geno_file = f'{genotype_list:ad}/' + geno_file\n",
    "    return path(geno_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Partition of the molecular phenotype for each genes\n",
    "This step extracts the molecular phenotype for each gene and transposes them into the formats needed in the follow-up analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[twas_fusion_1,compute_wgt_1,susie_1,susie_cv_1]\n",
    "\n",
    "input: molecular_pheno, for_each = \"regions\"\n",
    "output: f'{wd:a}/cache/{_input:bn}.{_regions[3]}.exp',\n",
    "        f'{wd:a}/cache/{_input:bn}.{_regions[3]}.pheno'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "    # Get the line number for the region in the file\n",
    "    line_num = system(\"awk '($$[region_name]==\\\"$[_regions[3]]\\\") {print NR}' $[_input]\", intern=T)\n",
    "    if (length(line_num) == 0){\n",
    "      stop( \"Cannot find $[_regions[3]] in column $[region_name]  $[_input]\")}\n",
    "    yi <- data.table::fread(file = $[_input:r], skip = as.integer(line_num) - 1, nrows = 1)\n",
    "    samplenames_yi <- data.table::fread(file = $[_input:r], skip = 0, nrows = 1)\n",
    "    colnames(yi) <- colnames(samplenames_yi)\n",
    "    readr::write_tsv(yi, path = \"$[_output[0]]\", na = \"NA\", append = FALSE, col_names = TRUE, quote_escape = \"double\")\n",
    "    yi <- as.data.frame(yi[, $[data_start]:ncol(yi), drop = FALSE])\n",
    "    yj <- rbind(colnames(yi),colnames(yi),yi)\n",
    "    readr::write_tsv(as.data.frame(t(yj)), path = \"$[_output[1]]\", na = \"NA\", append = FALSE, col_names = TRUE, quote_escape = \"double\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Construction of Plink trio for each gene\n",
    "This step constructs the plink file for each gene based on the output of previous steps. Specifically it:\n",
    "\n",
    "1. Selects only the SNPs within the start and end position of the corresponding region (gene)\n",
    "2. Replaces the Phenotype value (last column) of the .fam based on the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[twas_fusion_2,compute_wgt_2,susie_2,susie_cv_2]\n",
    "input: group_by = 2, group_with = 'regions'\n",
    "output: f'{_input[0]:n}.bed',\n",
    "        f'{_input[0]:n}.bim',\n",
    "        f'{_input[0]:n}.fam'\n",
    "\n",
    "# look up for genotype file\n",
    "geno_file = get_genotype_file(_regions[0],genotype_list,geno_inventory)\n",
    "\n",
    "parameter: extract_snp = f'{geno_file:an}.bim'\n",
    "parameter: exclude_snp = \"./.\"\n",
    "parameter: keep_sample = f'{_input[1]}'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container, volumes = [f'{geno_file:ad}:{geno_file:ad}']\n",
    "    ##### Get the locus genotypes for $[_regions[3]]\n",
    "    plink --bfile $[geno_file:an] \\\n",
    "    --pheno $[_input[1]] \\\n",
    "    --make-bed \\\n",
    "    --out $[_output[0]:n] \\\n",
    "    --chr $[_regions[0]] \\\n",
    "    --from-bp $[int(_regions[1]) - window ] \\\n",
    "    --to-bp $[int(_regions[2]) + window ] \\\n",
    "    --keep $[keep_sample] \\\n",
    "    --extract $[extract_snp]\n",
    "    --exclude $[exclude_snp]\n",
    "    --allow-no-sex || true\n",
    "    touch $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Computation of Weight\n",
    "This step computes the association between the phenotype and each of the SNPs. Specifically it:\n",
    "1. Estimates and documents the heritability (Ratio of genetics variance and phenotypic variance) of each region (gene)\n",
    "\n",
    "2. for those regions with significant heritability\n",
    "    1. Compute the association with the model specified for each SNPs\n",
    "    2. Stored the output in a wgt.RData for each region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#Actual weight computation \n",
    "[twas_fusion_3,compute_wgt_3]\n",
    "input: group_with = 'regions'\n",
    "output: f'{weights_path}/{_input[0]:bn}.wgt.RDat'\n",
    "import os\n",
    "skip_if(os.path.getsize(f'{_input[0]}') == 0)\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand= \"$[ ]\",stderr = f'{_output[0]:nn}.stderr', stdout = f'{_output[0]:nn}.stdout', container = container\n",
    "    Rscript /home/hs3163/GIT/neuro-twas/Workflow/FUSION-Copy1.compute_weights.R \\\n",
    "    --bfile $[_input[0]:n] \\\n",
    "    --tmp $[_input[0]:n].tmp \\\n",
    "    --out $[_output[0]:nn] \\\n",
    "    --verbose 0 \\\n",
    "    --save_hsq \\\n",
    "    --PATH_gcta `which gcta64` \\\n",
    "    --PATH_gemma `which gemma` \\\n",
    "    --models \"$[\",\".join(model)]\"  \\\n",
    "    --hsq_p $[hsq_p]\n",
    "    ## Creat dummy output file that will be deleted next step\n",
    "    touch $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Creation of weight index\n",
    "This step creates an index documenting the information about each of the weight data, including the filename, corresponding region ID, Chromosomes, start and end position, heritability, and the SE and P-value of the heritability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[twas_fusion_4,compute_wgt_4]\n",
    "input: group_by = \"all\"\n",
    "output: weights_list\n",
    "import os\n",
    "weight_files = [x for x in _input if not os.path.getsize(x) == 0]\n",
    "regions_dict = dict([(x[3], (x[0], x[1], x[2])) for x in regions])\n",
    "res = [[\"WGT\", \"ID\",\"CHR\",\"P0\",\"P1\",\"Heritability\",\"Heritability_SE\",\"Heritability_LRT_P_val\"]]\n",
    "for item in weight_files:\n",
    "    name = f'{item:bnn}'.lstrip(f'.{molecular_pheno:bn}')\n",
    "    hsq = open(f\"{item:nn}.hsq\").read().strip().split()\n",
    "    res.append([str(item),name,regions_dict[name][0], regions_dict[name][1], regions_dict[name][2], hsq[1], hsq[2], hsq[3]])\n",
    "with open(_output, 'w') as f:\n",
    "    f.write('\\n'.join(['\\t'.join(x) for x in res]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Conducting of association test\n",
    "This step conduct the association test by modifying SNPs' association and traits of interests with the association between SNPs and the molecular phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Association test, At least 20G of asso_mem is required.\n",
    "[twas_fusion_5, association_test]\n",
    "depends: weights_list\n",
    "chrom_list = list(set(get_output(f'cut -f 3 {weights_list} | tail -n+2').strip().split(\"\\n\")))\n",
    "# Path to GWAS summary statistics data (association results between SNP and disease in a GWAS)\n",
    "parameter: gwas_sumstat = path\n",
    "parameter: max_impute = 0.5\n",
    "parameter: asso_mem = \"20G\"\n",
    "input: gwas_sumstat, for_each = \"chrom_list\"\n",
    "output:f'{output_path}/{gwas_sumstat:bn}_chr{_chrom_list}.twas.txt'\n",
    "\n",
    "geno_file = get_genotype_file(_chrom_list, genotype_list, geno_inventory)\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h',  mem = asso_mem, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, volumes = [f'{geno_file:and}:{geno_file:and}']\n",
    "    FUSION.assoc_test.R \\\n",
    "    --sumstats $[_input] \\\n",
    "    --weights $[weights_list] \\\n",
    "    --weights_dir / \\\n",
    "    --ref_ld_chr $[geno_file:an] \\\n",
    "    --chr $[_chrom_list] \\\n",
    "    --max_impute $[max_impute] \\\n",
    "    --out $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Documentation of dummy file\n",
    "For the pipeline to run smoothly, dummy files were created in previous steps. This step finds all the dummy files and records them into a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Clean up dummy file\n",
    "[twas_fusion_6,compute_wgt_5]\n",
    "input: group_by = \"all\"\n",
    "output: f'{wd:a}/error_no_plink.log',\n",
    "        f'{wd:a}/error_no_wgt_computed.log',\n",
    "        f'{weights_path}/all_hsq.txt',\n",
    "        f'{output_path}/all_result.txt'\n",
    "bash: expand= \"$[ ]\"\n",
    "    find $[wd:a]/cache/*.bim -size 0 -print > $[_output[0]]\n",
    "    find $[weights_path]/*.wgt.RDat -size 0 -print > $[_output[1]]\n",
    "    echo filename heritability h_SE pval >  $[_output[2]]\n",
    "    cat $[weights_path]/*hsq >>  $[_output[2]]\n",
    "    cat $[output_path]/*twas.txt > $[_output[3]]\n",
    "    touch $[_output[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting kernel \"Bash\"\n",
      "/Users/haosun/Documents/WG_Reasearch_Assisstant/GIT/freshcopy/neuro-twas/Workflow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Conducting susieR test\n",
    "This step use the susieR packages to recompute the heritability and the weight of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#Susie test\n",
    "[susie_3,susie_cv_3]\n",
    "input:  group_by = 3, group_with = 'regions'\n",
    "output: f'{wd:a}/susie/{_input[0]:bn}.susie.hsq',\n",
    "        f'{wd:a}/susie/{_input[0]:bn}.wgt.txt',\n",
    "        f'{wd:a}/susie/{_input[0]:bn}.susie.model.RData'\n",
    "\n",
    "import os\n",
    "skip_if(os.path.getsize(f'{_input[0]}') == 0)\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h',  mem = \"6G\" , tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container,volumes = [f'{wd:a}:{wd:a}']\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"susieR\")\n",
    "    library(\"plink2R\")\n",
    "    library(\"readr\")\n",
    "    library(\"modelr\")\n",
    "    library(\"purrr\")\n",
    "\n",
    "    # Define functions\n",
    "    ###Functions to compute MAF and missing genotype rate\n",
    "    compute_maf <- function(geno){\n",
    "      f <- mean(geno,na.rm = TRUE)/2\n",
    "      return(min(f, 1-f))\n",
    "    }\n",
    "    \n",
    "    compute_missing <- function(geno){\n",
    "      miss <- sum(is.na(geno))/length(geno)\n",
    "      return(miss)\n",
    "    }\n",
    "    \n",
    "    mean_impute <- function(geno){\n",
    "      f <- apply(geno, 2, function(x) mean(x,na.rm = TRUE))\n",
    "      for (i in 1:length(f)) geno[,i][which(is.na(geno[,i]))] <- f[i]\n",
    "      return(geno)\n",
    "    }\n",
    "    \n",
    "    is_zero_variance <- function(x) {\n",
    "      if (length(unique(x%>%na.omit))==1) return(T)\n",
    "      else return(F)\n",
    "    }\n",
    "    ### Filter X matrix\n",
    "    filter_X <- function(X, missing_rate_thresh, maf_thresh) {\n",
    "      rm_col <- which(apply(X, 2, compute_missing) > missing_rate_thresh)\n",
    "      if (length(rm_col)) X <- X[, -rm_col]\n",
    "      rm_col <- which(apply(X, 2, compute_maf) < maf_thresh)\n",
    "      if (length(rm_col)) X <- X[, -rm_col]\n",
    "      rm_col <- which(apply(X, 2, is_zero_variance))\n",
    "      if (length(rm_col)) X <- X[, -rm_col]\n",
    "      return(mean_impute(X))}\n",
    "      \n",
    "    ###Function to impute the missing X with means and then scale and center X\n",
    "      impute_and_transform = function(genos,impute = TRUE){\n",
    "      tmp = genos\n",
    "      if(impute == TRUE){\n",
    "      for(i in 1:ncol(tmp)){\n",
    "        tmp[,i]=coalesce(tmp[,i],mean(tmp[,i]%>%na.omit()))%>%scale()}\n",
    "        return(tmp)\n",
    "      } else {\n",
    "    for(i in 1:ncol(tmp)){\n",
    "        tmp[,i]=tmp[,i]%>%scale()}\n",
    "        return(tmp)}}\n",
    "    \n",
    "\n",
    "    # Load data and transform\n",
    "    genos = read_plink(\"$[_input[0]:n]\")\n",
    "  \n",
    "    # Filter X by 0.1 NA and 0.01 MAF, and then transfrom X\n",
    "    \n",
    "    X_ftr = filter_X(genos$bed,0.1,0.01)\n",
    "    X = impute_and_transform(X_ftr)\n",
    "    \n",
    "    Y = genos$fam%>%as_tibble()%>%mutate(name = paste(V1,\":\",V2,sep = \"\"))\n",
    "    \n",
    "    # Make sure X and Y have the same order\n",
    "    Y = Y%>%arrange(match(name,rownames(X)))%>%select(V6)\n",
    "    \n",
    "    \n",
    "    # Center and scale Y \n",
    "    \n",
    "    Y = impute_and_transform(Y, impute = FALSE )\n",
    "    Y = Y$V6\n",
    "    \n",
    "    # Susie with full samples\n",
    "    fitted1 = susie(X, Y,\n",
    "                  L = $[causal_variables_L],\n",
    "                  estimate_residual_variance = TRUE, \n",
    "                  estimate_prior_variance = FALSE,\n",
    "                  scaled_prior_variance = $[scaled_prior_variance])\n",
    "                  \n",
    "    H1 = var(predict(fitted1))/var(Y)\n",
    "    \n",
    "    top_pip = which(fitted1$pip == fitted1$pip%>%max())\n",
    "    top_snp_full_sample = colnames(X)[top_pip]\n",
    "    top_snp_full_sample = paste(top_snp_full_sample,collapse = \",\")\n",
    "    \n",
    "    # hsq for the best performing snp using naive way, excluding only the na SNP\n",
    "    sumstats <- univariate_regression(X, Y)\n",
    "    z_scores <- sumstats$betahat / sumstats$sebetahat\n",
    "    max(abs(z_scores%>%na.omit()))\n",
    "    top_snp = which(abs(z_scores) == max(abs(z_scores%>%na.omit())))\n",
    "    top_snp\n",
    "    HH = var(X[,top_snp]*sumstats$betahat[top_snp])/var(Y)\n",
    "    top_snp_naive = colnames(X)[top_snp]\n",
    "    top_snp_naive = paste(top_snp_naive,collapse = \",\")\n",
    "\n",
    "    # storing all the HSQ\n",
    "    hsq_tbl = tibble(file = \"$[_input[0]:n]\", \n",
    "                    region = \"$[_regions[3]]\",\n",
    "                    CS_size = fitted1$sets$cs%>%length(),\n",
    "                    hsq_full_sample = H1,\n",
    "                    top_snp_full_sample = top_snp_full_sample,\n",
    "                    top_snp_naive = top_snp_naive)\n",
    "    hsq_tbl%>%write_delim(\"$[_output[0]]\",delim = \"\\t\")\n",
    "    # storing the wgt computed from susie, excluding only the na SNP\n",
    "    susie_wgt = tibble(susie = (coef(fitted1)[2:length(coef(fitted1))]))\n",
    "    susie_wgt%>%write_delim(\"$[_output[1]]\",delim = \"\\t\")\n",
    "    \n",
    "    # Save the model for future use\n",
    "    fitted1$X = X\n",
    "    fitted1$Y = Y\n",
    "    save(fitted1, file=\"$[_output[2]]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting susieR test\n",
    "This step conduct 100 times 5 fold cross validation for the susieR weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV with univariate susie\n",
    "[susie_cv_4]\n",
    "input: group_by = 3, group_with = 'regions'\n",
    "output:  f'{wd:a}/susie/{_input[2]:bn}.cv.RData',\n",
    "         f'{wd:a}/susie/{_input[2]:bn}.cv_diag.RData'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h',  mem = '8cG', tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand= \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"readr\")\n",
    "    library(\"plink2R\")\n",
    "    library(\"purrr\")\n",
    "    library(\"modelr\")\n",
    "    library(\"susieR\")\n",
    "    \n",
    "    # Define functions\n",
    "   \n",
    "    ## Compute rmse function\n",
    "    compute_rmse = function(raw,fitted){\n",
    "    rmse = rep(0,ncol(raw))\n",
    "    for (i in 1:ncol(raw)){\n",
    "      rmse[i] = ((fitted - raw)[,i])^2%>%mean(na.rm = TRUE)%>%sqrt() \n",
    "      }\n",
    "    return(rmse)\n",
    "    }\n",
    "    \n",
    "    ## Compute r2 function\n",
    "    compute_r2 = function(raw,fitted){\n",
    "      r2 = rep(0,ncol(raw))\n",
    "      for (j in 1:ncol(raw)){\n",
    "       r2[j] = summary(lm( as.matrix(fitted[,j]) ~ as.matrix(raw[,j]) ))$adj.r.sq\n",
    "      }\n",
    "      return(r2)\n",
    "    }\n",
    "    \n",
    "    ## Compute r2 raw\n",
    "    \n",
    "    compute_r2_raw = function(raw,fitted){\n",
    "      r2 = rep(0,ncol(raw))\n",
    "      for (j in 1:ncol(raw)){\n",
    "        r2[j] =  cor(as.matrix(fitted[,j])[which(!is.na(raw[,j]))],raw[,j]%>%na.omit())^2\n",
    "      }\n",
    "      return(r2)\n",
    "    }\n",
    "    \n",
    "    ## Get P.value\n",
    "    compute_pval = function(raw,fitted){\n",
    "      pval = rep(0,ncol(raw))\n",
    "      for (k in 1:ncol(raw)){\n",
    "        pval[k] = summary(lm( fitted[,k]%>%as.matrix ~ raw[,k]%>%as.matrix ))$coef[2,4]\n",
    "      }\n",
    "      return(pval)\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    ###Functions to compute MAF and missing genotype rate\n",
    "    compute_maf <- function(geno){\n",
    "      f <- mean(geno,na.rm = TRUE)/2\n",
    "      return(min(f, 1-f))\n",
    "    }\n",
    "    \n",
    "    compute_missing <- function(geno){\n",
    "      miss <- sum(is.na(geno))/length(geno)\n",
    "      return(miss)\n",
    "    }\n",
    "    \n",
    "    mean_impute <- function(geno){\n",
    "      f <- apply(geno, 2, function(x) mean(x,na.rm = TRUE))\n",
    "      for (i in 1:length(f)) geno[,i][which(is.na(geno[,i]))] <- f[i]\n",
    "      return(geno)\n",
    "    }\n",
    "    \n",
    "    is_zero_variance <- function(x) {\n",
    "      if (length(unique(x))==1) return(T)\n",
    "      else return(F)\n",
    "    }\n",
    "    ### Filter X matrix\n",
    "    filter_X <- function(X, missing_rate_thresh, maf_thresh) {\n",
    "      rm_col <- which(apply(X, 2, compute_missing) > missing_rate_thresh)\n",
    "      if (length(rm_col)) X <- X[, -rm_col]\n",
    "      rm_col <- which(apply(X, 2, compute_maf) < maf_thresh)\n",
    "      if (length(rm_col)) X <- X[, -rm_col]\n",
    "      rm_col <- which(apply(X, 2, is_zero_variance))\n",
    "      if (length(rm_col)) X <- X[, -rm_col]\n",
    "      return(mean_impute(X))\n",
    "    }\n",
    "    \n",
    "    ### Produce CV dataset\n",
    "    cv_data_gen = function(X,Y,times,test_prop){\n",
    "    # Merged the X and Y for producing testing and training set for modelr cv\n",
    "    cv_df_raw = cbind(X,Y)%>%as_tibble() \n",
    "    cv_df = crossv_mc(cv_df_raw, times ,test = test_prop)%>%mutate(\n",
    "      train_X = map(train,~as_tibble(.x)[1:ncol(X)]%>%as.matrix),\n",
    "      train_Y = map(train,~as_tibble(.x)[(ncol(X)+1):(ncol(X)+ncol(Y))]%>%as.matrix),\n",
    "      test_X = map(test,~as_tibble(.x)[1:ncol(X)]%>%as.matrix),\n",
    "      test_Y = map(test,~as_tibble(.x)[(ncol(X)+1):(ncol(X)+ncol(Y))]%>%as_tibble)\n",
    "    )  \n",
    "    \n",
    "    # Filter Train X with maf and missing, filter test X with the same col as Train X\n",
    "    cv_df = cv_df%>%mutate(\n",
    "    train_X = map(train_X,~filter_X(.x,0.1,0.01)),\n",
    "    test_X = map2(test_X,train_X,~.x%>%as_tibble()%>%select(colnames(.y))%>%as.matrix())\n",
    "    )\n",
    "    return(cv_df)\n",
    "    }\n",
    "    \n",
    "    # Load Data\n",
    "    full_model = attach('$[_input[2]]')\n",
    "    full_model = full_model$fitted1\n",
    "    X = full_model$X\n",
    "    Y = full_model$Y%>%as.tibble()\n",
    "\n",
    "    # Create cv dataset\n",
    "        \n",
    "    cv_df = cv_data_gen(X,Y,$[cv_times],$[testing_prop])\n",
    "\n",
    "    # Actual cv\n",
    "    \n",
    "    cv_df = cv_df%>%mutate(\n",
    "    \n",
    "   \n",
    "    ## Do susie\n",
    "    \n",
    "      susie = pmap(list(train_X,train_Y),function(first,second)(\n",
    "        \n",
    "        susie(first, second,\n",
    "        L = $[causal_variables_L],\n",
    "        estimate_residual_variance = TRUE, \n",
    "        estimate_prior_variance = FALSE,\n",
    "        scaled_prior_variance = $[scaled_prior_variance])\n",
    "        )))\n",
    "    \n",
    "    # Extract data \n",
    "    \n",
    "    cv_df = cv_df%>%mutate(\n",
    "      weight = map(susie,~\n",
    "      (coef(.x)[2:length(coef(.x))])\n",
    "      ),\n",
    "      test_fitted = map2(susie,test_X,~predict(.x,.y)%>%as_tibble),\n",
    "      rmse = map2(test_Y,test_fitted,~compute_rmse(.x,.y)),\n",
    "      r2 = map2(test_Y,test_fitted,~compute_r2(.x,.y)),\n",
    "      r2_raw = map2(test_Y,test_fitted,~compute_r2_raw(.x,.y)),\n",
    "      pval = map2(test_Y,test_fitted,~compute_pval(.x,.y))\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    \n",
    "    mean_rmse = cv_df%>%pull(rmse)%>%as.data.frame()%>%t()%>%as_tibble()%>%colMeans()\n",
    "    mean_r2 = cv_df%>%pull(r2)%>%as.data.frame()%>%t()%>%as_tibble()%>%colMeans()\n",
    "    mean_r2_raw = cv_df%>%pull(r2_raw)%>%as.data.frame()%>%t()%>%as_tibble()%>%colMeans()\n",
    "    mean_pval = cv_df%>%pull(pval)%>%as.data.frame()%>%t()%>%as_tibble()%>%colMeans()\n",
    "\n",
    "  \n",
    "    # Save metrics\n",
    "    full_model$rmse = mean_rmse\n",
    "    full_model$r2 = mean_r2 \n",
    "    full_model$r2_raw = mean_r2_raw    \n",
    "    full_model$pval = mean_pval    \n",
    "    fitted1 = full_model\n",
    "    # Save the CV data\n",
    "    save(cv_df,file = \"$[_output[1]]\")\n",
    "    \n",
    "    #Output\n",
    "    save(fitted1,file = \"$[_output[0]]\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save output\n",
    "This step create the all_hsq.txt file to summarize the susie result and create a R object to host all the susieR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create all_hsq for susie and saved all the susie object into one RData\n",
    "[susie_4]\n",
    "input: group_by = \"all\"\n",
    "output:f'{wd:a}/susie/all_hsq.txt',\n",
    "       f'{wd:a}/susie.RData'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand= \"$[ ]\"\n",
    "  head -1 $[_input[0]] > $[_output[0]]\n",
    "  cat $[wd:a]/susie/*.hsq | grep -v hsq_full_sample | uniq >> $[_output[0]]\n",
    "\n",
    "R: expand= \"$[ ]\", stderr = f'{_output[1]}.stderr', stdout = f'{_output[1]}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"readr\")\n",
    "    library(\"purrr\")\n",
    "    # Load a template\n",
    "    region = read_delim(\"$[_output[0]]\",delim =\"\\t\")%>%select(path = file, ID = region )\n",
    "    # get the path\n",
    "    dir = \"$[_input[0]:d]/\"\n",
    "    pre = \"$[_input[0]:bnnnnn]\"\n",
    "    sur = \".susie.model.RData\"\n",
    "    region = region%>%mutate(path = map(ID, ~paste(collapse = \"\", c(dir,pre,\".\",.x,sur))))\n",
    "    # Load the data\n",
    "    output = region%>%mutate(env = map(path,~attach(.x)),\n",
    "                             model = map(env, ~.x$fitted1))\n",
    "    # Save the combined output\n",
    "    save(output,file = \"$[_output[1]]\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create all_hsq for susie and saved all the susie object into one RData\n",
    "[susie_cv_5]\n",
    "input: group_by = \"all\"\n",
    "output:f'{wd:a}/susie/all_hsq.txt',\n",
    "       f'{wd:a}/susie_cv.RData'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h',  mem = '6G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand= \"$[ ]\"\n",
    "\n",
    "  head -1 $[_input[0]:nnn].hsq > $[_output[0]]\n",
    "  cat $[wd:a]/susie/*.hsq | grep -v hsq_full_sample | uniq >> $[_output[0]]\n",
    "\n",
    "R: expand= \"$[ ]\", stderr = f'{_output[1]}.stderr', stdout = f'{_output[1]}.stdout',container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"readr\")\n",
    "    library(\"purrr\")\n",
    "    # Load a template\n",
    "    region = read_delim(\"$[_output[0]]\",delim =\"\\t\")%>%select(path = file, ID = region )\n",
    "    # get the path\n",
    "    dir = \"$[_input[0]:d]/\"\n",
    "    pre = \"$[_input[0]:bnnnnn]\"\n",
    "    sur = \".susie.model.cv.RData\"\n",
    "    region = region%>%mutate(path = map(ID, ~paste(collapse = \"\", c(dir,pre,\".\",.x,sur))))\n",
    "    # Load the data\n",
    "    output = region%>%mutate(env = map(path,~attach(.x)),\n",
    "                             model = map(env, ~.x$fitted1))\n",
    "    # Save the combined output\n",
    "    save(output,file = \"$[_output[1]]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
